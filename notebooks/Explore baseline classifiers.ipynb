{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "147f2a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.base import clone\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7faf6cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_preprocess(path, seed, apply_ohe = True, scale = False):\n",
    "    \"\"\"\n",
    "    This function loads the data, splits the data into\n",
    "    training and test set and apply basic preprocessing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    conf: CfgNode\n",
    "        A yacs configuration node to access configuration data.\n",
    "    \"\"\"\n",
    "    TO_DROP = [\"site\", \"patient_ID\", \"infarct_side\", \"stroke\"]\n",
    "    TO_SCALE = [\"age\", \"stenosis_left\", \"stenosis_right\"]\n",
    "    TO_LABEL_ENCODE = [\"TIA\", \"hypertension\", \"cad\", \"gender\",\n",
    "                       \"diabetes\", \"hyperlipidemia\", \"smoker_status\", \n",
    "                       \"prs\", \"calcification\", \"at_least_4\"]\n",
    "    TO_OHE = [\"calcification_type_left\", \"calcification_type_right\"]\n",
    "    PREFIXES = [\"calcification_left\", \"calcification_right\"]    \n",
    "    TARGET = \"symptoms\"\n",
    "    \n",
    "    train_filename = 'train'\n",
    "    test_filename = 'test'\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    df[\"smoker_status\"] = df[\"smoker_status\"].apply(lambda v: \"no\" if v == \"no\" else \"yes\")\n",
    "    df[\"calcification_type_left\"] = df[\"calcification_type_left\"].apply(lambda v: v.replace(\" \", \"\"))\n",
    "    df[\"calcification_type_right\"] = df[\"calcification_type_right\"].apply(lambda v: v.replace(\" \", \"\"))\n",
    "\n",
    "    # drop unuseful features\n",
    "    df = df.drop(TO_DROP, axis=1)\n",
    "\n",
    "    X, y = df.drop(TARGET, axis=1), df[TARGET]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=seed,\n",
    "                                                        stratify=y)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    oe = OrdinalEncoder()\n",
    "\n",
    "    # start pre-processing\n",
    "    to_encode = TO_LABEL_ENCODE\n",
    "    if apply_ohe:\n",
    "        X_train = pd.get_dummies(X_train, columns=TO_OHE, prefix=PREFIXES)\n",
    "        X_test = pd.get_dummies(X_test, columns=TO_OHE, prefix=PREFIXES)\n",
    "    else:\n",
    "        to_encode += TO_OHE\n",
    "\n",
    "        train_filename += '_no_ohe'\n",
    "        test_filename += '_no_ohe'\n",
    "\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        X_train[TO_SCALE] = scaler.fit_transform(X_train[TO_SCALE])\n",
    "        X_test[TO_SCALE] = scaler.transform(X_test[TO_SCALE])\n",
    "    else:\n",
    "        train_filename += '_no_scale'\n",
    "        test_filename += '_no_scale'\n",
    "\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    X_train[to_encode] = oe.fit_transform(X_train[to_encode]).astype(np.uint8)\n",
    "\n",
    "    # test set\n",
    "    y_test = le.transform(y_test)\n",
    "    X_test[to_encode] = oe.transform(X_test[to_encode]).astype(np.uint8)\n",
    "\n",
    "    # add target vectors to feature matrices\n",
    "    #X_train[TARGET] = y_train\n",
    "    #X_test[TARGET] = y_test\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51e9220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocess(path, seed, apply_ohe = True, scale = False):\n",
    "    \"\"\"\n",
    "    This function loads the data, splits the data into\n",
    "    training and test set and apply basic preprocessing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    conf: CfgNode\n",
    "        A yacs configuration node to access configuration data.\n",
    "    \"\"\"\n",
    "    TO_DROP = [\"site\", \"patient_ID\", \"infarct_side\", \"stroke\"]\n",
    "    TO_SCALE = [\"age\", \"stenosis_left\", \"stenosis_right\"]\n",
    "    TO_LABEL_ENCODE = [\"TIA\", \"hypertension\", \"cad\", \"gender\",\n",
    "                       \"diabetes\", \"hyperlipidemia\", \"smoker_status\", \n",
    "                       \"prs\", \"calcification\", \"at_least_4\"]\n",
    "    TO_OHE = [\"calcification_type_left\", \"calcification_type_right\"]\n",
    "    PREFIXES = [\"calcification_left\", \"calcification_right\"]    \n",
    "    TARGET = \"symptoms\"\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    df[\"smoker_status\"] = df[\"smoker_status\"].apply(lambda v: \"no\" if v == \"no\" else \"yes\")\n",
    "    df[\"calcification_type_left\"] = df[\"calcification_type_left\"].apply(lambda v: v.replace(\" \", \"\"))\n",
    "    df[\"calcification_type_right\"] = df[\"calcification_type_right\"].apply(lambda v: v.replace(\" \", \"\"))\n",
    "\n",
    "    # drop unuseful features\n",
    "    df = df.drop(TO_DROP, axis=1)\n",
    "\n",
    "    X, y = df.drop(TARGET, axis=1), df[TARGET]\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    oe = OrdinalEncoder()\n",
    "\n",
    "    # start pre-processing\n",
    "    to_encode = TO_LABEL_ENCODE\n",
    "    if apply_ohe:\n",
    "        X = pd.get_dummies(X, columns=TO_OHE, prefix=PREFIXES)\n",
    "    else:\n",
    "        to_encode += [\"calcification_type_left\", \"calcification_type_right\"]\n",
    "\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        X[TO_SCALE] = scaler.fit_transform(X[TO_SCALE])\n",
    "\n",
    "    y = le.fit_transform(y)\n",
    "    X[to_encode] = oe.fit_transform(X[to_encode]).astype(np.uint8)\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9f52e0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 1, Train: 0.704 | Val: 0.6 | Test: 0.707\n",
      "Seed: 2, Train: 0.764 | Val: 0.656 | Test: 0.652\n",
      "Seed: 3, Train: 0.784 | Val: 0.676 | Test: 0.652\n",
      "Seed: 4, Train: 0.783 | Val: 0.662 | Test: 0.58\n",
      "Seed: 5, Train: 0.743 | Val: 0.648 | Test: 0.64\n",
      "Seed: 6, Train: 0.762 | Val: 0.661 | Test: 0.634\n",
      "Seed: 7, Train: 0.774 | Val: 0.667 | Test: 0.691\n",
      "Seed: 8, Train: 0.762 | Val: 0.673 | Test: 0.646\n",
      "Seed: 9, Train: 0.742 | Val: 0.66 | Test: 0.603\n",
      "Seed: 10, Train: 0.731 | Val: 0.643 | Test: 0.66\n",
      "Seed: 11, Train: 0.75 | Val: 0.627 | Test: 0.673\n",
      "Seed: 12, Train: 0.754 | Val: 0.663 | Test: 0.624\n",
      "Seed: 13, Train: 0.762 | Val: 0.653 | Test: 0.659\n",
      "Seed: 14, Train: 0.745 | Val: 0.631 | Test: 0.702\n",
      "Seed: 15, Train: 0.749 | Val: 0.646 | Test: 0.705\n",
      "Seed: 16, Train: 0.756 | Val: 0.651 | Test: 0.666\n",
      "Seed: 17, Train: 0.762 | Val: 0.662 | Test: 0.657\n",
      "Seed: 18, Train: 0.745 | Val: 0.626 | Test: 0.738\n",
      "Seed: 19, Train: 0.739 | Val: 0.636 | Test: 0.697\n",
      "Seed: 20, Train: 0.752 | Val: 0.649 | Test: 0.683\n",
      "Seed: 21, Train: 0.718 | Val: 0.63 | Test: 0.711\n",
      "Seed: 22, Train: 0.756 | Val: 0.66 | Test: 0.671\n",
      "Seed: 23, Train: 0.769 | Val: 0.657 | Test: 0.621\n",
      "Seed: 24, Train: 0.747 | Val: 0.652 | Test: 0.683\n",
      "Seed: 25, Train: 0.736 | Val: 0.665 | Test: 0.67\n",
      "Seed: 26, Train: 0.743 | Val: 0.67 | Test: 0.568\n",
      "Seed: 27, Train: 0.738 | Val: 0.652 | Test: 0.699\n",
      "Seed: 28, Train: 0.746 | Val: 0.638 | Test: 0.658\n",
      "Seed: 29, Train: 0.707 | Val: 0.607 | Test: 0.51\n",
      "Seed: 30, Train: 0.757 | Val: 0.666 | Test: 0.634\n",
      "Seed: 31, Train: 0.751 | Val: 0.644 | Test: 0.668\n",
      "Seed: 32, Train: 0.767 | Val: 0.665 | Test: 0.638\n",
      "Seed: 33, Train: 0.745 | Val: 0.658 | Test: 0.678\n",
      "Seed: 34, Train: 0.745 | Val: 0.644 | Test: 0.644\n",
      "Seed: 35, Train: 0.758 | Val: 0.673 | Test: 0.603\n",
      "Seed: 36, Train: 0.724 | Val: 0.647 | Test: 0.664\n",
      "Seed: 37, Train: 0.755 | Val: 0.663 | Test: 0.706\n",
      "Seed: 38, Train: 0.758 | Val: 0.656 | Test: 0.624\n",
      "Seed: 39, Train: 0.788 | Val: 0.676 | Test: 0.631\n",
      "Seed: 40, Train: 0.736 | Val: 0.644 | Test: 0.734\n",
      "Seed: 41, Train: 0.768 | Val: 0.675 | Test: 0.585\n",
      "Seed: 42, Train: 0.768 | Val: 0.67 | Test: 0.62\n",
      "Seed: 43, Train: 0.773 | Val: 0.638 | Test: 0.691\n",
      "Seed: 44, Train: 0.75 | Val: 0.657 | Test: 0.655\n",
      "Seed: 45, Train: 0.749 | Val: 0.652 | Test: 0.694\n",
      "Seed: 46, Train: 0.765 | Val: 0.663 | Test: 0.64\n",
      "Seed: 47, Train: 0.741 | Val: 0.637 | Test: 0.702\n",
      "Seed: 48, Train: 0.751 | Val: 0.643 | Test: 0.666\n",
      "Seed: 49, Train: 0.759 | Val: 0.665 | Test: 0.669\n",
      "Seed: 50, Train: 0.718 | Val: 0.634 | Test: 0.7\n",
      "Seed: 51, Train: 0.779 | Val: 0.664 | Test: 0.627\n",
      "Seed: 52, Train: 0.781 | Val: 0.675 | Test: 0.597\n",
      "Seed: 53, Train: 0.753 | Val: 0.636 | Test: 0.717\n",
      "Seed: 54, Train: 0.769 | Val: 0.669 | Test: 0.644\n",
      "Seed: 55, Train: 0.768 | Val: 0.667 | Test: 0.656\n",
      "Seed: 56, Train: 0.75 | Val: 0.645 | Test: 0.678\n",
      "Seed: 57, Train: 0.721 | Val: 0.646 | Test: 0.72\n",
      "Seed: 58, Train: 0.769 | Val: 0.678 | Test: 0.624\n",
      "Seed: 59, Train: 0.758 | Val: 0.649 | Test: 0.624\n",
      "Seed: 60, Train: 0.762 | Val: 0.631 | Test: 0.735\n",
      "Seed: 61, Train: 0.744 | Val: 0.659 | Test: 0.701\n",
      "Seed: 62, Train: 0.702 | Val: 0.645 | Test: 0.669\n",
      "Seed: 63, Train: 0.74 | Val: 0.642 | Test: 0.633\n",
      "Seed: 64, Train: 0.763 | Val: 0.663 | Test: 0.675\n",
      "Seed: 65, Train: 0.712 | Val: 0.627 | Test: 0.732\n",
      "Seed: 66, Train: 0.762 | Val: 0.661 | Test: 0.55\n",
      "Seed: 67, Train: 0.775 | Val: 0.683 | Test: 0.614\n",
      "Seed: 68, Train: 0.768 | Val: 0.663 | Test: 0.664\n",
      "Seed: 69, Train: 0.762 | Val: 0.655 | Test: 0.643\n",
      "Seed: 70, Train: 0.718 | Val: 0.614 | Test: 0.738\n",
      "Seed: 71, Train: 0.761 | Val: 0.661 | Test: 0.65\n",
      "Seed: 72, Train: 0.749 | Val: 0.645 | Test: 0.695\n",
      "Seed: 73, Train: 0.783 | Val: 0.688 | Test: 0.655\n",
      "Seed: 74, Train: 0.764 | Val: 0.673 | Test: 0.62\n",
      "Seed: 75, Train: 0.732 | Val: 0.654 | Test: 0.666\n",
      "Seed: 76, Train: 0.736 | Val: 0.645 | Test: 0.612\n",
      "Seed: 77, Train: 0.771 | Val: 0.685 | Test: 0.63\n",
      "Seed: 78, Train: 0.758 | Val: 0.67 | Test: 0.631\n",
      "Seed: 79, Train: 0.748 | Val: 0.661 | Test: 0.585\n",
      "Seed: 80, Train: 0.732 | Val: 0.656 | Test: 0.656\n",
      "Seed: 81, Train: 0.755 | Val: 0.659 | Test: 0.699\n",
      "Seed: 82, Train: 0.752 | Val: 0.67 | Test: 0.629\n",
      "Seed: 83, Train: 0.739 | Val: 0.658 | Test: 0.699\n",
      "Seed: 84, Train: 0.76 | Val: 0.653 | Test: 0.65\n",
      "Seed: 85, Train: 0.752 | Val: 0.663 | Test: 0.651\n",
      "Seed: 86, Train: 0.732 | Val: 0.622 | Test: 0.71\n",
      "Seed: 87, Train: 0.756 | Val: 0.655 | Test: 0.625\n",
      "Seed: 88, Train: 0.727 | Val: 0.67 | Test: 0.633\n",
      "Seed: 89, Train: 0.732 | Val: 0.65 | Test: 0.625\n",
      "Seed: 90, Train: 0.742 | Val: 0.674 | Test: 0.568\n",
      "Seed: 91, Train: 0.771 | Val: 0.662 | Test: 0.607\n",
      "Seed: 92, Train: 0.737 | Val: 0.66 | Test: 0.582\n",
      "Seed: 93, Train: 0.748 | Val: 0.665 | Test: 0.682\n",
      "Seed: 94, Train: 0.771 | Val: 0.678 | Test: 0.637\n",
      "Seed: 95, Train: 0.76 | Val: 0.683 | Test: 0.593\n",
      "Seed: 96, Train: 0.738 | Val: 0.661 | Test: 0.703\n",
      "Seed: 97, Train: 0.753 | Val: 0.674 | Test: 0.646\n",
      "Seed: 98, Train: 0.764 | Val: 0.648 | Test: 0.684\n",
      "Seed: 99, Train: 0.774 | Val: 0.666 | Test: 0.66\n",
      "Best is seed 73 with train AUC 0.783 and test AUC 0.688 \n"
     ]
    }
   ],
   "source": [
    "# 0.68, seed 58\n",
    "train_scores_dict = {}\n",
    "test_scores_dict = {}\n",
    "\n",
    "for seed in range(1, 100):\n",
    "    X_train, y_train, X_test, y_test = load_split_preprocess(path = '../input/calcifications.csv',\n",
    "                                                             seed=seed,\n",
    "                                                             apply_ohe = False,\n",
    "                                                             scale=False)\n",
    "    \n",
    "    cv = RepeatedStratifiedKFold(n_repeats=10, n_splits=10, random_state=seed)\n",
    "    \n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(50,),\n",
    "                    solver='adam',\n",
    "                    activation=\"relu\",\n",
    "                    learning_rate=\"adaptive\",\n",
    "                    learning_rate_init=0.05,\n",
    "                    max_iter=500,\n",
    "                    #early_stopping=True,\n",
    "                    alpha=0.03,\n",
    "                    random_state=seed)\n",
    "    \n",
    "    #mlp_copy = clone(mlp)\n",
    "    #mlp_copy.predict = mlp_copy.predict_proba\n",
    "    \n",
    "    scores = cross_validate(mlp, \n",
    "                         X_train, y_train, \n",
    "                         scoring=\"roc_auc\", \n",
    "                         cv=cv,\n",
    "                         return_train_score=True,\n",
    "                         n_jobs=-1)\n",
    "    \n",
    "    curr_auc_train = np.median(scores[\"train_score\"]).round(3)\n",
    "    curr_auc_test = np.median(scores[\"test_score\"]).round(3)    \n",
    "    train_scores_dict[seed] = curr_auc_train\n",
    "    test_scores_dict[seed] = curr_auc_test\n",
    "    \n",
    "    mlp.fit(X_train, y_train)\n",
    "    probas = mlp.predict_proba(X_test)\n",
    "    auc = roc_auc_score(y_test, probas[:, 1]).round(3)\n",
    "    \n",
    "    print(\"Seed: {}, Train: {} | Val: {} | Test: {}\".format(seed, curr_auc_train, curr_auc_test, auc))\n",
    "\n",
    "best_seed = max(test_scores_dict, key=test_scores_dict.get)\n",
    "print(\"Best is seed {} with train AUC {} and test AUC {} \".format(best_seed, \n",
    "                                                                  train_scores_dict[best_seed],\n",
    "                                                                  test_scores_dict[best_seed]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d5dcd003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 58, Train: 0.786 | Val: 0.686\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "BEST_SEED = 58\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_split_preprocess(path = '../input/calcifications.csv',\n",
    "                                                         seed=BEST_SEED,\n",
    "                                                         apply_ohe = False,\n",
    "                                                         scale=False)\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_repeats=10, n_splits=10, random_state=BEST_SEED)\n",
    "    \n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50,),\n",
    "                solver='adam',\n",
    "                activation=\"relu\",\n",
    "                learning_rate=\"adaptive\",\n",
    "                learning_rate_init=0.02,\n",
    "                max_iter=500,\n",
    "                #early_stopping=True,\n",
    "                alpha=0.03,\n",
    "                random_state=BEST_SEED)\n",
    "\n",
    "#mlp_copy = clone(mlp)\n",
    "#mlp_copy.predict = mlp_copy.predict_proba\n",
    "\n",
    "scores = cross_validate(mlp, \n",
    "                     X_train, y_train, \n",
    "                     scoring=\"roc_auc\", \n",
    "                     cv=cv,\n",
    "                     return_train_score=True,\n",
    "                     n_jobs=-1)\n",
    "\n",
    "curr_auc_train = np.median(scores[\"train_score\"]).round(3)\n",
    "curr_auc_test = np.median(scores[\"test_score\"]).round(3) \n",
    "\n",
    "print(\"Seed: {}, Train: {} | Val: {}\".format(BEST_SEED, curr_auc_train, curr_auc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4b4e8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 1, Train: 0.718 | Val: 0.648\n",
      "Seed: 2, Train: 0.74 | Val: 0.66\n",
      "Seed: 3, Train: 0.746 | Val: 0.658\n",
      "Seed: 4, Train: 0.752 | Val: 0.675\n",
      "Seed: 5, Train: 0.719 | Val: 0.647\n",
      "Seed: 6, Train: 0.739 | Val: 0.663\n",
      "Seed: 7, Train: 0.754 | Val: 0.674\n",
      "Seed: 8, Train: 0.73 | Val: 0.667\n",
      "Seed: 9, Train: 0.729 | Val: 0.672\n",
      "Seed: 10, Train: 0.726 | Val: 0.66\n",
      "Seed: 11, Train: 0.741 | Val: 0.657\n",
      "Seed: 12, Train: 0.729 | Val: 0.661\n",
      "Seed: 13, Train: 0.738 | Val: 0.671\n",
      "Seed: 14, Train: 0.738 | Val: 0.645\n",
      "Seed: 15, Train: 0.746 | Val: 0.673\n",
      "Seed: 16, Train: 0.741 | Val: 0.664\n",
      "Seed: 17, Train: 0.738 | Val: 0.675\n",
      "Seed: 18, Train: 0.737 | Val: 0.672\n",
      "Seed: 19, Train: 0.729 | Val: 0.654\n",
      "Seed: 20, Train: 0.736 | Val: 0.654\n",
      "Seed: 21, Train: 0.719 | Val: 0.66\n",
      "Seed: 22, Train: 0.743 | Val: 0.676\n",
      "Seed: 23, Train: 0.747 | Val: 0.66\n",
      "Seed: 24, Train: 0.738 | Val: 0.663\n",
      "Seed: 25, Train: 0.72 | Val: 0.653\n",
      "Seed: 26, Train: 0.72 | Val: 0.656\n",
      "Seed: 27, Train: 0.737 | Val: 0.669\n",
      "Seed: 28, Train: 0.733 | Val: 0.666\n",
      "Seed: 29, Train: 0.682 | Val: 0.636\n",
      "Seed: 30, Train: 0.738 | Val: 0.664\n",
      "Seed: 31, Train: 0.743 | Val: 0.666\n",
      "Seed: 32, Train: 0.747 | Val: 0.681\n",
      "Seed: 33, Train: 0.741 | Val: 0.663\n",
      "Seed: 34, Train: 0.735 | Val: 0.659\n",
      "Seed: 35, Train: 0.731 | Val: 0.655\n",
      "Seed: 36, Train: 0.717 | Val: 0.666\n",
      "Seed: 37, Train: 0.742 | Val: 0.658\n",
      "Seed: 38, Train: 0.736 | Val: 0.657\n",
      "Seed: 39, Train: 0.748 | Val: 0.663\n",
      "Seed: 40, Train: 0.729 | Val: 0.659\n",
      "Seed: 41, Train: 0.739 | Val: 0.648\n",
      "Seed: 42, Train: 0.739 | Val: 0.669\n",
      "Seed: 43, Train: 0.749 | Val: 0.673\n",
      "Seed: 44, Train: 0.722 | Val: 0.659\n",
      "Seed: 45, Train: 0.738 | Val: 0.681\n",
      "Seed: 46, Train: 0.745 | Val: 0.679\n",
      "Seed: 47, Train: 0.74 | Val: 0.661\n",
      "Seed: 48, Train: 0.745 | Val: 0.665\n",
      "Seed: 49, Train: 0.735 | Val: 0.659\n",
      "Seed: 50, Train: 0.712 | Val: 0.66\n",
      "Seed: 51, Train: 0.745 | Val: 0.666\n",
      "Seed: 52, Train: 0.75 | Val: 0.672\n",
      "Seed: 53, Train: 0.732 | Val: 0.663\n",
      "Seed: 54, Train: 0.743 | Val: 0.671\n",
      "Seed: 55, Train: 0.736 | Val: 0.654\n",
      "Seed: 56, Train: 0.735 | Val: 0.664\n",
      "Seed: 57, Train: 0.722 | Val: 0.665\n",
      "Seed: 58, Train: 0.737 | Val: 0.665\n",
      "Seed: 59, Train: 0.74 | Val: 0.651\n",
      "Seed: 60, Train: 0.747 | Val: 0.655\n",
      "Seed: 61, Train: 0.729 | Val: 0.671\n",
      "Seed: 62, Train: 0.705 | Val: 0.659\n",
      "Seed: 63, Train: 0.74 | Val: 0.658\n",
      "Seed: 64, Train: 0.742 | Val: 0.666\n",
      "Seed: 65, Train: 0.734 | Val: 0.658\n",
      "Seed: 66, Train: 0.741 | Val: 0.663\n",
      "Seed: 67, Train: 0.74 | Val: 0.678\n",
      "Seed: 68, Train: 0.738 | Val: 0.677\n",
      "Seed: 69, Train: 0.75 | Val: 0.661\n",
      "Seed: 70, Train: 0.724 | Val: 0.654\n",
      "Seed: 71, Train: 0.744 | Val: 0.667\n",
      "Seed: 72, Train: 0.741 | Val: 0.675\n",
      "Seed: 73, Train: 0.746 | Val: 0.675\n",
      "Seed: 74, Train: 0.746 | Val: 0.672\n",
      "Seed: 75, Train: 0.717 | Val: 0.664\n",
      "Seed: 76, Train: 0.734 | Val: 0.663\n",
      "Seed: 77, Train: 0.732 | Val: 0.67\n",
      "Seed: 78, Train: 0.742 | Val: 0.664\n",
      "Seed: 79, Train: 0.733 | Val: 0.667\n",
      "Seed: 80, Train: 0.719 | Val: 0.671\n",
      "Seed: 81, Train: 0.732 | Val: 0.651\n",
      "Seed: 82, Train: 0.73 | Val: 0.669\n",
      "Seed: 83, Train: 0.735 | Val: 0.677\n",
      "Seed: 84, Train: 0.74 | Val: 0.666\n",
      "Seed: 85, Train: 0.736 | Val: 0.672\n",
      "Seed: 86, Train: 0.734 | Val: 0.655\n",
      "Seed: 87, Train: 0.723 | Val: 0.657\n",
      "Seed: 88, Train: 0.714 | Val: 0.652\n",
      "Seed: 89, Train: 0.719 | Val: 0.666\n",
      "Seed: 90, Train: 0.719 | Val: 0.673\n",
      "Seed: 91, Train: 0.745 | Val: 0.676\n",
      "Seed: 92, Train: 0.73 | Val: 0.658\n",
      "Seed: 93, Train: 0.733 | Val: 0.66\n",
      "Seed: 94, Train: 0.738 | Val: 0.659\n",
      "Seed: 95, Train: 0.739 | Val: 0.661\n",
      "Seed: 96, Train: 0.742 | Val: 0.672\n",
      "Seed: 97, Train: 0.74 | Val: 0.668\n",
      "Seed: 98, Train: 0.745 | Val: 0.666\n",
      "Seed: 99, Train: 0.753 | Val: 0.661\n",
      "Seed: 100, Train: 0.737 | Val: 0.671\n",
      "Seed: 101, Train: 0.739 | Val: 0.654\n",
      "Seed: 102, Train: 0.727 | Val: 0.656\n",
      "Seed: 103, Train: 0.726 | Val: 0.663\n",
      "Seed: 104, Train: 0.729 | Val: 0.671\n",
      "Seed: 105, Train: 0.725 | Val: 0.66\n",
      "Seed: 106, Train: 0.732 | Val: 0.659\n",
      "Seed: 107, Train: 0.738 | Val: 0.666\n",
      "Seed: 108, Train: 0.753 | Val: 0.675\n",
      "Seed: 109, Train: 0.742 | Val: 0.669\n",
      "Seed: 110, Train: 0.752 | Val: 0.643\n",
      "Seed: 111, Train: 0.748 | Val: 0.666\n",
      "Seed: 112, Train: 0.728 | Val: 0.66\n",
      "Seed: 113, Train: 0.726 | Val: 0.66\n",
      "Seed: 114, Train: 0.736 | Val: 0.664\n",
      "Seed: 115, Train: 0.722 | Val: 0.663\n",
      "Seed: 116, Train: 0.745 | Val: 0.675\n",
      "Seed: 117, Train: 0.726 | Val: 0.66\n",
      "Seed: 118, Train: 0.725 | Val: 0.662\n",
      "Seed: 119, Train: 0.741 | Val: 0.656\n",
      "Seed: 120, Train: 0.753 | Val: 0.669\n",
      "Seed: 121, Train: 0.745 | Val: 0.662\n",
      "Seed: 122, Train: 0.721 | Val: 0.674\n",
      "Seed: 123, Train: 0.742 | Val: 0.669\n",
      "Seed: 124, Train: 0.728 | Val: 0.644\n",
      "Seed: 125, Train: 0.728 | Val: 0.658\n",
      "Seed: 126, Train: 0.741 | Val: 0.676\n",
      "Seed: 127, Train: 0.728 | Val: 0.671\n",
      "Seed: 128, Train: 0.719 | Val: 0.631\n",
      "Seed: 129, Train: 0.722 | Val: 0.648\n",
      "Seed: 130, Train: 0.741 | Val: 0.65\n",
      "Seed: 131, Train: 0.74 | Val: 0.666\n",
      "Seed: 132, Train: 0.733 | Val: 0.664\n",
      "Seed: 133, Train: 0.722 | Val: 0.673\n",
      "Seed: 134, Train: 0.729 | Val: 0.658\n",
      "Seed: 135, Train: 0.74 | Val: 0.675\n",
      "Seed: 136, Train: 0.738 | Val: 0.651\n",
      "Seed: 137, Train: 0.727 | Val: 0.659\n",
      "Seed: 138, Train: 0.733 | Val: 0.645\n",
      "Seed: 139, Train: 0.705 | Val: 0.651\n",
      "Seed: 140, Train: 0.745 | Val: 0.681\n",
      "Seed: 141, Train: 0.743 | Val: 0.665\n",
      "Seed: 142, Train: 0.745 | Val: 0.666\n",
      "Seed: 143, Train: 0.734 | Val: 0.657\n",
      "Seed: 144, Train: 0.744 | Val: 0.663\n",
      "Seed: 145, Train: 0.733 | Val: 0.675\n",
      "Seed: 146, Train: 0.736 | Val: 0.658\n",
      "Seed: 147, Train: 0.734 | Val: 0.658\n",
      "Seed: 148, Train: 0.732 | Val: 0.644\n",
      "Seed: 149, Train: 0.741 | Val: 0.679\n",
      "Seed: 150, Train: 0.741 | Val: 0.669\n",
      "Seed: 151, Train: 0.74 | Val: 0.665\n",
      "Seed: 152, Train: 0.742 | Val: 0.66\n",
      "Seed: 153, Train: 0.744 | Val: 0.659\n",
      "Seed: 154, Train: 0.729 | Val: 0.682\n",
      "Seed: 155, Train: 0.727 | Val: 0.677\n",
      "Seed: 156, Train: 0.742 | Val: 0.668\n",
      "Seed: 157, Train: 0.742 | Val: 0.675\n",
      "Seed: 158, Train: 0.727 | Val: 0.657\n",
      "Seed: 159, Train: 0.749 | Val: 0.665\n",
      "Seed: 160, Train: 0.761 | Val: 0.667\n",
      "Seed: 161, Train: 0.728 | Val: 0.675\n",
      "Seed: 162, Train: 0.742 | Val: 0.658\n",
      "Seed: 163, Train: 0.715 | Val: 0.661\n",
      "Seed: 164, Train: 0.749 | Val: 0.649\n",
      "Seed: 165, Train: 0.754 | Val: 0.678\n",
      "Seed: 166, Train: 0.74 | Val: 0.666\n",
      "Seed: 167, Train: 0.739 | Val: 0.667\n",
      "Seed: 168, Train: 0.738 | Val: 0.667\n",
      "Seed: 169, Train: 0.727 | Val: 0.652\n",
      "Seed: 170, Train: 0.706 | Val: 0.652\n",
      "Seed: 171, Train: 0.743 | Val: 0.681\n",
      "Seed: 172, Train: 0.732 | Val: 0.682\n",
      "Seed: 173, Train: 0.723 | Val: 0.657\n",
      "Seed: 174, Train: 0.725 | Val: 0.655\n",
      "Seed: 175, Train: 0.746 | Val: 0.661\n",
      "Seed: 176, Train: 0.732 | Val: 0.651\n",
      "Seed: 177, Train: 0.742 | Val: 0.666\n",
      "Seed: 178, Train: 0.742 | Val: 0.68\n",
      "Seed: 179, Train: 0.751 | Val: 0.659\n",
      "Seed: 180, Train: 0.747 | Val: 0.66\n",
      "Seed: 181, Train: 0.757 | Val: 0.675\n",
      "Seed: 182, Train: 0.735 | Val: 0.667\n",
      "Seed: 183, Train: 0.729 | Val: 0.666\n",
      "Seed: 184, Train: 0.743 | Val: 0.648\n",
      "Seed: 185, Train: 0.748 | Val: 0.681\n",
      "Seed: 186, Train: 0.721 | Val: 0.663\n",
      "Seed: 187, Train: 0.739 | Val: 0.667\n",
      "Seed: 188, Train: 0.72 | Val: 0.669\n",
      "Seed: 189, Train: 0.739 | Val: 0.667\n",
      "Seed: 190, Train: 0.741 | Val: 0.66\n",
      "Seed: 191, Train: 0.741 | Val: 0.668\n",
      "Seed: 192, Train: 0.726 | Val: 0.668\n",
      "Seed: 193, Train: 0.756 | Val: 0.669\n",
      "Seed: 194, Train: 0.743 | Val: 0.656\n",
      "Seed: 195, Train: 0.748 | Val: 0.672\n",
      "Seed: 196, Train: 0.746 | Val: 0.687\n",
      "Seed: 197, Train: 0.738 | Val: 0.677\n",
      "Seed: 198, Train: 0.72 | Val: 0.668\n",
      "Seed: 199, Train: 0.731 | Val: 0.659\n",
      "Seed: 200, Train: 0.739 | Val: 0.659\n",
      "Seed: 201, Train: 0.737 | Val: 0.655\n",
      "Seed: 202, Train: 0.731 | Val: 0.656\n",
      "Seed: 203, Train: 0.733 | Val: 0.672\n",
      "Seed: 204, Train: 0.743 | Val: 0.669\n",
      "Seed: 205, Train: 0.725 | Val: 0.675\n",
      "Seed: 206, Train: 0.744 | Val: 0.67\n",
      "Seed: 207, Train: 0.724 | Val: 0.659\n",
      "Seed: 208, Train: 0.722 | Val: 0.651\n",
      "Seed: 209, Train: 0.737 | Val: 0.657\n",
      "Seed: 210, Train: 0.729 | Val: 0.685\n",
      "Seed: 211, Train: 0.749 | Val: 0.664\n",
      "Seed: 212, Train: 0.744 | Val: 0.664\n",
      "Seed: 213, Train: 0.747 | Val: 0.661\n",
      "Seed: 214, Train: 0.732 | Val: 0.658\n",
      "Seed: 215, Train: 0.722 | Val: 0.661\n",
      "Seed: 216, Train: 0.73 | Val: 0.662\n",
      "Seed: 217, Train: 0.72 | Val: 0.681\n",
      "Seed: 218, Train: 0.745 | Val: 0.67\n",
      "Seed: 219, Train: 0.756 | Val: 0.667\n",
      "Seed: 220, Train: 0.743 | Val: 0.672\n",
      "Seed: 221, Train: 0.727 | Val: 0.669\n",
      "Seed: 222, Train: 0.753 | Val: 0.667\n",
      "Seed: 223, Train: 0.713 | Val: 0.652\n",
      "Seed: 224, Train: 0.738 | Val: 0.665\n",
      "Seed: 225, Train: 0.732 | Val: 0.657\n",
      "Seed: 226, Train: 0.749 | Val: 0.676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 227, Train: 0.739 | Val: 0.666\n",
      "Seed: 228, Train: 0.73 | Val: 0.657\n",
      "Seed: 229, Train: 0.726 | Val: 0.66\n",
      "Seed: 230, Train: 0.747 | Val: 0.679\n",
      "Seed: 231, Train: 0.755 | Val: 0.677\n",
      "Seed: 232, Train: 0.754 | Val: 0.674\n",
      "Seed: 233, Train: 0.737 | Val: 0.665\n",
      "Seed: 234, Train: 0.762 | Val: 0.651\n",
      "Seed: 235, Train: 0.739 | Val: 0.664\n",
      "Seed: 236, Train: 0.746 | Val: 0.665\n",
      "Seed: 237, Train: 0.744 | Val: 0.676\n",
      "Seed: 238, Train: 0.73 | Val: 0.651\n",
      "Seed: 239, Train: 0.723 | Val: 0.666\n",
      "Seed: 240, Train: 0.747 | Val: 0.676\n",
      "Seed: 241, Train: 0.741 | Val: 0.656\n",
      "Seed: 242, Train: 0.736 | Val: 0.674\n",
      "Seed: 243, Train: 0.707 | Val: 0.647\n",
      "Seed: 244, Train: 0.739 | Val: 0.67\n",
      "Seed: 245, Train: 0.742 | Val: 0.665\n",
      "Seed: 246, Train: 0.75 | Val: 0.674\n",
      "Seed: 247, Train: 0.735 | Val: 0.672\n",
      "Seed: 248, Train: 0.733 | Val: 0.647\n",
      "Seed: 249, Train: 0.735 | Val: 0.667\n",
      "Seed: 250, Train: 0.742 | Val: 0.674\n",
      "Seed: 251, Train: 0.734 | Val: 0.665\n",
      "Seed: 252, Train: 0.734 | Val: 0.664\n",
      "Seed: 253, Train: 0.74 | Val: 0.653\n",
      "Seed: 254, Train: 0.724 | Val: 0.66\n",
      "Seed: 255, Train: 0.727 | Val: 0.659\n",
      "Seed: 256, Train: 0.742 | Val: 0.657\n",
      "Seed: 257, Train: 0.734 | Val: 0.669\n",
      "Seed: 258, Train: 0.743 | Val: 0.661\n",
      "Seed: 259, Train: 0.737 | Val: 0.672\n",
      "Seed: 260, Train: 0.732 | Val: 0.656\n",
      "Seed: 261, Train: 0.727 | Val: 0.654\n",
      "Seed: 262, Train: 0.738 | Val: 0.677\n",
      "Seed: 263, Train: 0.711 | Val: 0.669\n",
      "Seed: 264, Train: 0.723 | Val: 0.668\n",
      "Seed: 265, Train: 0.703 | Val: 0.66\n",
      "Seed: 266, Train: 0.74 | Val: 0.676\n",
      "Seed: 267, Train: 0.742 | Val: 0.68\n",
      "Seed: 268, Train: 0.696 | Val: 0.636\n",
      "Seed: 269, Train: 0.727 | Val: 0.67\n",
      "Seed: 270, Train: 0.748 | Val: 0.675\n",
      "Seed: 271, Train: 0.741 | Val: 0.674\n",
      "Seed: 272, Train: 0.756 | Val: 0.675\n",
      "Seed: 273, Train: 0.739 | Val: 0.67\n",
      "Seed: 274, Train: 0.737 | Val: 0.651\n",
      "Seed: 275, Train: 0.724 | Val: 0.667\n",
      "Seed: 276, Train: 0.737 | Val: 0.67\n",
      "Seed: 277, Train: 0.75 | Val: 0.682\n",
      "Seed: 278, Train: 0.743 | Val: 0.677\n",
      "Seed: 279, Train: 0.716 | Val: 0.663\n",
      "Seed: 280, Train: 0.745 | Val: 0.673\n",
      "Seed: 281, Train: 0.728 | Val: 0.663\n",
      "Seed: 282, Train: 0.725 | Val: 0.667\n",
      "Seed: 283, Train: 0.734 | Val: 0.656\n",
      "Seed: 284, Train: 0.722 | Val: 0.669\n",
      "Seed: 285, Train: 0.72 | Val: 0.66\n",
      "Seed: 286, Train: 0.725 | Val: 0.663\n",
      "Seed: 287, Train: 0.732 | Val: 0.657\n",
      "Seed: 288, Train: 0.744 | Val: 0.668\n",
      "Seed: 289, Train: 0.747 | Val: 0.678\n",
      "Seed: 290, Train: 0.756 | Val: 0.673\n",
      "Seed: 291, Train: 0.738 | Val: 0.666\n",
      "Seed: 292, Train: 0.756 | Val: 0.668\n",
      "Seed: 293, Train: 0.741 | Val: 0.672\n",
      "Seed: 294, Train: 0.743 | Val: 0.658\n",
      "Seed: 295, Train: 0.728 | Val: 0.656\n",
      "Seed: 296, Train: 0.751 | Val: 0.671\n",
      "Seed: 297, Train: 0.748 | Val: 0.662\n",
      "Seed: 298, Train: 0.719 | Val: 0.674\n",
      "Seed: 299, Train: 0.75 | Val: 0.66\n",
      "Best is seed 196 with train AUC 0.746 and test AUC 0.687 \n"
     ]
    }
   ],
   "source": [
    "# MLPClassifier\n",
    "\n",
    "train_scores_dict = {}\n",
    "test_scores_dict = {}\n",
    "\n",
    "for seed in range(1, 300):\n",
    "    X, y = load_preprocess(path = '../input/calcifications.csv',\n",
    "                                                             seed=seed,\n",
    "                                                             apply_ohe = False,\n",
    "                                                             scale=False)\n",
    "    \n",
    "    cv = RepeatedStratifiedKFold(n_repeats=10, n_splits=10, random_state=seed)\n",
    "    \n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(50,),\n",
    "                    solver='adam',\n",
    "                    activation=\"relu\",\n",
    "                    learning_rate=\"adaptive\",\n",
    "                    learning_rate_init=0.05,\n",
    "                    max_iter=500,\n",
    "                    #early_stopping=True,\n",
    "                    alpha=0.03,\n",
    "                    random_state=seed)\n",
    "    \n",
    "    #mlp_copy = clone(mlp)\n",
    "    #mlp_copy.predict = mlp_copy.predict_proba\n",
    "    \n",
    "    scores = cross_validate(mlp, \n",
    "                         X, y, \n",
    "                         scoring=\"roc_auc\", \n",
    "                         cv=cv,\n",
    "                         return_train_score=True,\n",
    "                         n_jobs=-1)\n",
    "    \n",
    "    curr_auc_train = np.median(scores[\"train_score\"]).round(3)\n",
    "    curr_auc_test = np.median(scores[\"test_score\"]).round(3)    \n",
    "    train_scores_dict[seed] = curr_auc_train\n",
    "    test_scores_dict[seed] = curr_auc_test\n",
    "    \n",
    "    print(\"Seed: {}, Train: {} | Val: {}\".format(seed, curr_auc_train, curr_auc_test))\n",
    "\n",
    "best_seed = max(test_scores_dict, key=test_scores_dict.get)\n",
    "print(\"Best is seed {} with train AUC {} and test AUC {} \".format(best_seed, \n",
    "                                                                  train_scores_dict[best_seed],\n",
    "                                                                  test_scores_dict[best_seed]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "201810fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 1, Train: 0.794 | Val: 0.714\n",
      "Seed: 2, Train: 0.794 | Val: 0.695\n",
      "Seed: 3, Train: 0.794 | Val: 0.709\n",
      "Seed: 4, Train: 0.793 | Val: 0.699\n",
      "Seed: 5, Train: 0.794 | Val: 0.709\n",
      "Seed: 6, Train: 0.793 | Val: 0.713\n",
      "Seed: 7, Train: 0.793 | Val: 0.71\n",
      "Seed: 8, Train: 0.794 | Val: 0.709\n",
      "Seed: 9, Train: 0.797 | Val: 0.711\n",
      "Seed: 10, Train: 0.798 | Val: 0.708\n",
      "Seed: 11, Train: 0.794 | Val: 0.718\n",
      "Seed: 12, Train: 0.795 | Val: 0.706\n",
      "Seed: 13, Train: 0.794 | Val: 0.71\n",
      "Seed: 14, Train: 0.794 | Val: 0.702\n",
      "Seed: 15, Train: 0.795 | Val: 0.705\n",
      "Seed: 16, Train: 0.795 | Val: 0.716\n",
      "Seed: 17, Train: 0.792 | Val: 0.715\n",
      "Seed: 18, Train: 0.794 | Val: 0.715\n",
      "Seed: 19, Train: 0.795 | Val: 0.71\n",
      "Seed: 20, Train: 0.795 | Val: 0.715\n",
      "Seed: 21, Train: 0.794 | Val: 0.715\n",
      "Seed: 22, Train: 0.792 | Val: 0.718\n",
      "Seed: 23, Train: 0.795 | Val: 0.712\n",
      "Seed: 24, Train: 0.794 | Val: 0.704\n",
      "Seed: 25, Train: 0.795 | Val: 0.711\n",
      "Seed: 26, Train: 0.793 | Val: 0.706\n",
      "Seed: 27, Train: 0.796 | Val: 0.713\n",
      "Seed: 28, Train: 0.794 | Val: 0.706\n",
      "Seed: 29, Train: 0.794 | Val: 0.711\n",
      "Seed: 30, Train: 0.795 | Val: 0.718\n",
      "Seed: 31, Train: 0.793 | Val: 0.703\n",
      "Seed: 32, Train: 0.793 | Val: 0.71\n",
      "Seed: 33, Train: 0.794 | Val: 0.715\n",
      "Seed: 34, Train: 0.792 | Val: 0.714\n",
      "Seed: 35, Train: 0.795 | Val: 0.711\n",
      "Seed: 36, Train: 0.793 | Val: 0.705\n",
      "Seed: 37, Train: 0.793 | Val: 0.712\n",
      "Seed: 38, Train: 0.794 | Val: 0.699\n",
      "Seed: 39, Train: 0.796 | Val: 0.71\n",
      "Seed: 40, Train: 0.794 | Val: 0.711\n",
      "Seed: 41, Train: 0.794 | Val: 0.71\n",
      "Seed: 42, Train: 0.793 | Val: 0.713\n",
      "Seed: 43, Train: 0.793 | Val: 0.71\n",
      "Seed: 44, Train: 0.794 | Val: 0.704\n",
      "Seed: 45, Train: 0.796 | Val: 0.709\n",
      "Seed: 46, Train: 0.796 | Val: 0.702\n",
      "Seed: 47, Train: 0.793 | Val: 0.71\n",
      "Seed: 48, Train: 0.794 | Val: 0.704\n",
      "Seed: 49, Train: 0.795 | Val: 0.71\n",
      "Seed: 50, Train: 0.796 | Val: 0.711\n",
      "Seed: 51, Train: 0.795 | Val: 0.707\n",
      "Seed: 52, Train: 0.792 | Val: 0.714\n",
      "Seed: 53, Train: 0.794 | Val: 0.718\n",
      "Seed: 54, Train: 0.793 | Val: 0.713\n",
      "Seed: 55, Train: 0.794 | Val: 0.707\n",
      "Seed: 56, Train: 0.794 | Val: 0.714\n",
      "Seed: 57, Train: 0.793 | Val: 0.714\n",
      "Seed: 58, Train: 0.794 | Val: 0.707\n",
      "Seed: 59, Train: 0.793 | Val: 0.7\n",
      "Seed: 60, Train: 0.795 | Val: 0.7\n",
      "Seed: 61, Train: 0.794 | Val: 0.713\n",
      "Seed: 62, Train: 0.793 | Val: 0.707\n",
      "Seed: 63, Train: 0.795 | Val: 0.712\n",
      "Seed: 64, Train: 0.793 | Val: 0.711\n",
      "Seed: 65, Train: 0.793 | Val: 0.708\n",
      "Seed: 66, Train: 0.793 | Val: 0.704\n",
      "Seed: 67, Train: 0.793 | Val: 0.711\n",
      "Seed: 68, Train: 0.794 | Val: 0.718\n",
      "Seed: 69, Train: 0.794 | Val: 0.716\n",
      "Seed: 70, Train: 0.795 | Val: 0.709\n",
      "Seed: 71, Train: 0.795 | Val: 0.711\n",
      "Seed: 72, Train: 0.794 | Val: 0.709\n",
      "Seed: 73, Train: 0.797 | Val: 0.716\n",
      "Seed: 74, Train: 0.794 | Val: 0.712\n",
      "Seed: 75, Train: 0.794 | Val: 0.71\n",
      "Seed: 76, Train: 0.795 | Val: 0.705\n",
      "Seed: 77, Train: 0.794 | Val: 0.706\n",
      "Seed: 78, Train: 0.796 | Val: 0.719\n",
      "Seed: 79, Train: 0.792 | Val: 0.713\n",
      "Seed: 80, Train: 0.793 | Val: 0.7\n",
      "Seed: 81, Train: 0.795 | Val: 0.704\n",
      "Seed: 82, Train: 0.794 | Val: 0.705\n",
      "Seed: 83, Train: 0.792 | Val: 0.71\n",
      "Seed: 84, Train: 0.793 | Val: 0.709\n",
      "Seed: 85, Train: 0.793 | Val: 0.71\n",
      "Seed: 86, Train: 0.794 | Val: 0.708\n",
      "Seed: 87, Train: 0.794 | Val: 0.711\n",
      "Seed: 88, Train: 0.791 | Val: 0.716\n",
      "Seed: 89, Train: 0.794 | Val: 0.717\n",
      "Seed: 90, Train: 0.793 | Val: 0.71\n",
      "Seed: 91, Train: 0.794 | Val: 0.716\n",
      "Seed: 92, Train: 0.793 | Val: 0.702\n",
      "Seed: 93, Train: 0.795 | Val: 0.716\n",
      "Seed: 94, Train: 0.793 | Val: 0.714\n",
      "Seed: 95, Train: 0.793 | Val: 0.706\n",
      "Seed: 96, Train: 0.793 | Val: 0.714\n",
      "Seed: 97, Train: 0.794 | Val: 0.714\n",
      "Seed: 98, Train: 0.794 | Val: 0.712\n",
      "Seed: 99, Train: 0.794 | Val: 0.707\n",
      "Seed: 100, Train: 0.795 | Val: 0.72\n",
      "Seed: 101, Train: 0.794 | Val: 0.717\n",
      "Seed: 102, Train: 0.794 | Val: 0.705\n",
      "Seed: 103, Train: 0.795 | Val: 0.708\n",
      "Seed: 104, Train: 0.794 | Val: 0.707\n",
      "Seed: 105, Train: 0.793 | Val: 0.714\n",
      "Seed: 106, Train: 0.794 | Val: 0.717\n",
      "Seed: 107, Train: 0.793 | Val: 0.706\n",
      "Seed: 108, Train: 0.795 | Val: 0.714\n",
      "Seed: 109, Train: 0.795 | Val: 0.706\n",
      "Seed: 110, Train: 0.795 | Val: 0.709\n",
      "Seed: 111, Train: 0.793 | Val: 0.713\n",
      "Seed: 112, Train: 0.793 | Val: 0.707\n",
      "Seed: 113, Train: 0.794 | Val: 0.712\n",
      "Seed: 114, Train: 0.793 | Val: 0.713\n",
      "Seed: 115, Train: 0.793 | Val: 0.709\n",
      "Seed: 116, Train: 0.795 | Val: 0.705\n",
      "Seed: 117, Train: 0.795 | Val: 0.707\n",
      "Seed: 118, Train: 0.794 | Val: 0.716\n",
      "Seed: 119, Train: 0.793 | Val: 0.704\n",
      "Seed: 120, Train: 0.792 | Val: 0.71\n",
      "Seed: 121, Train: 0.795 | Val: 0.719\n",
      "Seed: 122, Train: 0.792 | Val: 0.715\n",
      "Seed: 123, Train: 0.795 | Val: 0.718\n",
      "Seed: 124, Train: 0.791 | Val: 0.711\n",
      "Seed: 125, Train: 0.794 | Val: 0.709\n",
      "Seed: 126, Train: 0.792 | Val: 0.71\n",
      "Seed: 127, Train: 0.793 | Val: 0.717\n",
      "Seed: 128, Train: 0.794 | Val: 0.708\n",
      "Seed: 129, Train: 0.792 | Val: 0.707\n",
      "Seed: 130, Train: 0.795 | Val: 0.706\n",
      "Seed: 131, Train: 0.794 | Val: 0.706\n",
      "Seed: 132, Train: 0.793 | Val: 0.706\n",
      "Seed: 133, Train: 0.794 | Val: 0.716\n",
      "Seed: 134, Train: 0.792 | Val: 0.714\n",
      "Seed: 135, Train: 0.793 | Val: 0.706\n",
      "Seed: 136, Train: 0.796 | Val: 0.707\n",
      "Seed: 137, Train: 0.795 | Val: 0.703\n",
      "Seed: 138, Train: 0.794 | Val: 0.709\n",
      "Seed: 139, Train: 0.792 | Val: 0.703\n",
      "Seed: 140, Train: 0.793 | Val: 0.71\n",
      "Seed: 141, Train: 0.794 | Val: 0.704\n",
      "Seed: 142, Train: 0.795 | Val: 0.707\n",
      "Seed: 143, Train: 0.794 | Val: 0.708\n",
      "Seed: 144, Train: 0.794 | Val: 0.703\n",
      "Seed: 145, Train: 0.794 | Val: 0.709\n",
      "Seed: 146, Train: 0.795 | Val: 0.709\n",
      "Seed: 147, Train: 0.796 | Val: 0.714\n",
      "Seed: 148, Train: 0.793 | Val: 0.712\n",
      "Seed: 149, Train: 0.795 | Val: 0.703\n",
      "Seed: 150, Train: 0.795 | Val: 0.711\n",
      "Seed: 151, Train: 0.794 | Val: 0.714\n",
      "Seed: 152, Train: 0.794 | Val: 0.712\n",
      "Seed: 153, Train: 0.794 | Val: 0.704\n",
      "Seed: 154, Train: 0.793 | Val: 0.71\n",
      "Seed: 155, Train: 0.794 | Val: 0.714\n",
      "Seed: 156, Train: 0.794 | Val: 0.717\n",
      "Seed: 157, Train: 0.796 | Val: 0.711\n",
      "Seed: 158, Train: 0.794 | Val: 0.707\n",
      "Seed: 159, Train: 0.793 | Val: 0.711\n",
      "Seed: 160, Train: 0.796 | Val: 0.71\n",
      "Seed: 161, Train: 0.793 | Val: 0.709\n",
      "Seed: 162, Train: 0.793 | Val: 0.713\n",
      "Seed: 163, Train: 0.793 | Val: 0.704\n",
      "Seed: 164, Train: 0.792 | Val: 0.701\n",
      "Seed: 165, Train: 0.793 | Val: 0.703\n",
      "Seed: 166, Train: 0.794 | Val: 0.71\n",
      "Seed: 167, Train: 0.796 | Val: 0.704\n",
      "Seed: 168, Train: 0.795 | Val: 0.705\n",
      "Seed: 169, Train: 0.795 | Val: 0.705\n",
      "Seed: 170, Train: 0.792 | Val: 0.708\n",
      "Seed: 171, Train: 0.795 | Val: 0.713\n",
      "Seed: 172, Train: 0.792 | Val: 0.72\n",
      "Seed: 173, Train: 0.795 | Val: 0.716\n",
      "Seed: 174, Train: 0.791 | Val: 0.711\n",
      "Seed: 175, Train: 0.796 | Val: 0.719\n",
      "Seed: 176, Train: 0.794 | Val: 0.7\n",
      "Seed: 177, Train: 0.794 | Val: 0.709\n",
      "Seed: 178, Train: 0.795 | Val: 0.705\n",
      "Seed: 179, Train: 0.794 | Val: 0.711\n",
      "Seed: 180, Train: 0.792 | Val: 0.711\n",
      "Seed: 181, Train: 0.795 | Val: 0.715\n",
      "Seed: 182, Train: 0.794 | Val: 0.705\n",
      "Seed: 183, Train: 0.794 | Val: 0.712\n",
      "Seed: 184, Train: 0.792 | Val: 0.71\n",
      "Seed: 185, Train: 0.795 | Val: 0.704\n",
      "Seed: 186, Train: 0.794 | Val: 0.708\n",
      "Seed: 187, Train: 0.794 | Val: 0.713\n",
      "Seed: 188, Train: 0.794 | Val: 0.711\n",
      "Seed: 189, Train: 0.793 | Val: 0.709\n",
      "Seed: 190, Train: 0.795 | Val: 0.706\n",
      "Seed: 191, Train: 0.795 | Val: 0.702\n",
      "Seed: 192, Train: 0.794 | Val: 0.705\n",
      "Seed: 193, Train: 0.793 | Val: 0.699\n",
      "Seed: 194, Train: 0.791 | Val: 0.714\n",
      "Seed: 195, Train: 0.795 | Val: 0.705\n",
      "Seed: 196, Train: 0.795 | Val: 0.716\n",
      "Seed: 197, Train: 0.794 | Val: 0.71\n",
      "Seed: 198, Train: 0.793 | Val: 0.709\n",
      "Seed: 199, Train: 0.795 | Val: 0.706\n",
      "Seed: 200, Train: 0.793 | Val: 0.708\n",
      "Seed: 201, Train: 0.793 | Val: 0.712\n",
      "Seed: 202, Train: 0.794 | Val: 0.711\n",
      "Seed: 203, Train: 0.796 | Val: 0.711\n",
      "Seed: 204, Train: 0.794 | Val: 0.707\n",
      "Seed: 205, Train: 0.795 | Val: 0.705\n",
      "Seed: 206, Train: 0.794 | Val: 0.706\n",
      "Seed: 207, Train: 0.794 | Val: 0.71\n",
      "Seed: 208, Train: 0.796 | Val: 0.701\n",
      "Seed: 209, Train: 0.796 | Val: 0.706\n",
      "Seed: 210, Train: 0.794 | Val: 0.71\n",
      "Seed: 211, Train: 0.794 | Val: 0.705\n",
      "Seed: 212, Train: 0.794 | Val: 0.703\n",
      "Seed: 213, Train: 0.794 | Val: 0.718\n",
      "Seed: 214, Train: 0.794 | Val: 0.717\n",
      "Seed: 215, Train: 0.796 | Val: 0.714\n",
      "Seed: 216, Train: 0.794 | Val: 0.709\n",
      "Seed: 217, Train: 0.792 | Val: 0.716\n",
      "Seed: 218, Train: 0.794 | Val: 0.707\n",
      "Seed: 219, Train: 0.795 | Val: 0.708\n",
      "Seed: 220, Train: 0.793 | Val: 0.707\n",
      "Seed: 221, Train: 0.793 | Val: 0.705\n",
      "Seed: 222, Train: 0.796 | Val: 0.712\n",
      "Seed: 223, Train: 0.795 | Val: 0.71\n",
      "Seed: 224, Train: 0.794 | Val: 0.702\n",
      "Seed: 225, Train: 0.795 | Val: 0.715\n",
      "Seed: 226, Train: 0.794 | Val: 0.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 227, Train: 0.794 | Val: 0.708\n",
      "Seed: 228, Train: 0.794 | Val: 0.714\n",
      "Seed: 229, Train: 0.794 | Val: 0.704\n",
      "Seed: 230, Train: 0.795 | Val: 0.708\n",
      "Seed: 231, Train: 0.795 | Val: 0.71\n",
      "Seed: 232, Train: 0.794 | Val: 0.71\n",
      "Seed: 233, Train: 0.792 | Val: 0.706\n",
      "Seed: 234, Train: 0.795 | Val: 0.713\n",
      "Seed: 235, Train: 0.794 | Val: 0.724\n",
      "Seed: 236, Train: 0.794 | Val: 0.71\n",
      "Seed: 237, Train: 0.794 | Val: 0.703\n",
      "Seed: 238, Train: 0.796 | Val: 0.714\n",
      "Seed: 239, Train: 0.792 | Val: 0.708\n",
      "Seed: 240, Train: 0.793 | Val: 0.711\n",
      "Seed: 241, Train: 0.792 | Val: 0.707\n",
      "Seed: 242, Train: 0.794 | Val: 0.711\n",
      "Seed: 243, Train: 0.792 | Val: 0.708\n",
      "Seed: 244, Train: 0.793 | Val: 0.718\n",
      "Seed: 245, Train: 0.794 | Val: 0.708\n",
      "Seed: 246, Train: 0.793 | Val: 0.712\n",
      "Seed: 247, Train: 0.794 | Val: 0.711\n",
      "Seed: 248, Train: 0.793 | Val: 0.703\n",
      "Seed: 249, Train: 0.794 | Val: 0.709\n",
      "Seed: 250, Train: 0.793 | Val: 0.709\n",
      "Seed: 251, Train: 0.795 | Val: 0.706\n",
      "Seed: 252, Train: 0.793 | Val: 0.704\n",
      "Seed: 253, Train: 0.793 | Val: 0.706\n",
      "Seed: 254, Train: 0.795 | Val: 0.721\n",
      "Seed: 255, Train: 0.794 | Val: 0.696\n",
      "Seed: 256, Train: 0.792 | Val: 0.702\n",
      "Seed: 257, Train: 0.795 | Val: 0.707\n",
      "Seed: 258, Train: 0.796 | Val: 0.707\n",
      "Seed: 259, Train: 0.794 | Val: 0.714\n",
      "Seed: 260, Train: 0.795 | Val: 0.699\n",
      "Seed: 261, Train: 0.792 | Val: 0.708\n",
      "Seed: 262, Train: 0.794 | Val: 0.711\n",
      "Seed: 263, Train: 0.793 | Val: 0.708\n",
      "Seed: 264, Train: 0.796 | Val: 0.702\n",
      "Seed: 265, Train: 0.796 | Val: 0.716\n",
      "Seed: 266, Train: 0.793 | Val: 0.711\n",
      "Seed: 267, Train: 0.794 | Val: 0.703\n",
      "Seed: 268, Train: 0.791 | Val: 0.701\n",
      "Seed: 269, Train: 0.796 | Val: 0.716\n",
      "Seed: 270, Train: 0.794 | Val: 0.697\n",
      "Seed: 271, Train: 0.795 | Val: 0.706\n",
      "Seed: 272, Train: 0.793 | Val: 0.704\n",
      "Seed: 273, Train: 0.793 | Val: 0.713\n",
      "Seed: 274, Train: 0.795 | Val: 0.704\n",
      "Seed: 275, Train: 0.795 | Val: 0.706\n",
      "Seed: 276, Train: 0.794 | Val: 0.719\n",
      "Seed: 277, Train: 0.795 | Val: 0.711\n",
      "Seed: 278, Train: 0.793 | Val: 0.706\n",
      "Seed: 279, Train: 0.794 | Val: 0.707\n",
      "Seed: 280, Train: 0.795 | Val: 0.703\n",
      "Seed: 281, Train: 0.794 | Val: 0.711\n",
      "Seed: 282, Train: 0.795 | Val: 0.709\n",
      "Seed: 283, Train: 0.794 | Val: 0.717\n",
      "Seed: 284, Train: 0.794 | Val: 0.708\n",
      "Seed: 285, Train: 0.794 | Val: 0.712\n",
      "Seed: 286, Train: 0.795 | Val: 0.713\n",
      "Seed: 287, Train: 0.793 | Val: 0.713\n",
      "Seed: 288, Train: 0.794 | Val: 0.7\n",
      "Seed: 289, Train: 0.791 | Val: 0.707\n",
      "Seed: 290, Train: 0.792 | Val: 0.713\n",
      "Seed: 291, Train: 0.794 | Val: 0.708\n",
      "Seed: 292, Train: 0.794 | Val: 0.716\n",
      "Seed: 293, Train: 0.794 | Val: 0.71\n",
      "Seed: 294, Train: 0.794 | Val: 0.705\n",
      "Seed: 295, Train: 0.795 | Val: 0.708\n",
      "Seed: 296, Train: 0.793 | Val: 0.712\n",
      "Seed: 297, Train: 0.794 | Val: 0.708\n",
      "Seed: 298, Train: 0.792 | Val: 0.705\n",
      "Seed: 299, Train: 0.793 | Val: 0.71\n",
      "Best is seed 235 with train AUC 0.794 and test AUC 0.724 \n"
     ]
    }
   ],
   "source": [
    "# 0.68, seed 58\n",
    "train_scores_dict = {}\n",
    "test_scores_dict = {}\n",
    "\n",
    "for seed in range(1, 300):\n",
    "    X, y = load_preprocess(path = '../input/calcifications.csv',\n",
    "                                                             seed=seed,\n",
    "                                                             apply_ohe = False,\n",
    "                                                             scale=False)\n",
    "    \n",
    "    cv = RepeatedStratifiedKFold(n_repeats=10, n_splits=10, random_state=seed)\n",
    "    \n",
    "    lgbm = LGBMClassifier(objective=\"binary\",\n",
    "                        metric=\"auc\",\n",
    "                        boosting_type=\"gbdt\",\n",
    "                        seed=seed,\n",
    "                        #enable_categorical=True,\n",
    "                        feature_pre_filter=False,\n",
    "                        force_row_wise=True,\n",
    "                        deterministic=True,\n",
    "                        learning_rate=0.05,\n",
    "                        is_unbalance=True,\n",
    "                        verbosity=-1,\n",
    "                        reg_alpha=7,\n",
    "                        reg_lambda=7,\n",
    "                        max_depth=10,\n",
    "                        n_estimators=150,\n",
    "                        colsample_bytree=0.7,                          \n",
    "                        n_jobs=1)\n",
    "    \n",
    "    #mlp_copy = clone(mlp)\n",
    "    #mlp_copy.predict = mlp_copy.predict_proba\n",
    "    \n",
    "    scores = cross_validate(lgbm, \n",
    "                         X, y, \n",
    "                         scoring=\"roc_auc\", \n",
    "                         cv=cv,\n",
    "                         return_train_score=True,\n",
    "                         n_jobs=-1)\n",
    "    \n",
    "    curr_auc_train = np.median(scores[\"train_score\"]).round(3)\n",
    "    curr_auc_test = np.median(scores[\"test_score\"]).round(3)    \n",
    "    train_scores_dict[seed] = curr_auc_train\n",
    "    test_scores_dict[seed] = curr_auc_test\n",
    "    \n",
    "    print(\"Seed: {}, Train: {} | Val: {}\".format(seed, curr_auc_train, curr_auc_test))\n",
    "\n",
    "best_seed = max(test_scores_dict, key=test_scores_dict.get)\n",
    "print(\"Best is seed {} with train AUC {} and test AUC {} \".format(best_seed, \n",
    "                                                                  train_scores_dict[best_seed],\n",
    "                                                                  test_scores_dict[best_seed]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b67830e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 100, Train: 0.77 | Val: 0.701\n",
      "Seed: 101, Train: 0.771 | Val: 0.695\n",
      "Seed: 102, Train: 0.772 | Val: 0.691\n",
      "Seed: 103, Train: 0.771 | Val: 0.697\n",
      "Seed: 104, Train: 0.769 | Val: 0.69\n",
      "Seed: 105, Train: 0.768 | Val: 0.692\n",
      "Seed: 106, Train: 0.769 | Val: 0.7\n",
      "Seed: 107, Train: 0.771 | Val: 0.695\n",
      "Seed: 108, Train: 0.772 | Val: 0.701\n",
      "Seed: 109, Train: 0.77 | Val: 0.692\n",
      "Seed: 110, Train: 0.77 | Val: 0.688\n",
      "Seed: 111, Train: 0.768 | Val: 0.692\n",
      "Seed: 112, Train: 0.768 | Val: 0.693\n",
      "Seed: 113, Train: 0.77 | Val: 0.697\n",
      "Seed: 114, Train: 0.766 | Val: 0.683\n",
      "Seed: 115, Train: 0.767 | Val: 0.69\n",
      "Seed: 116, Train: 0.767 | Val: 0.682\n",
      "Seed: 117, Train: 0.769 | Val: 0.69\n",
      "Seed: 118, Train: 0.765 | Val: 0.692\n",
      "Seed: 119, Train: 0.77 | Val: 0.694\n",
      "Seed: 120, Train: 0.768 | Val: 0.696\n",
      "Seed: 121, Train: 0.768 | Val: 0.695\n",
      "Seed: 122, Train: 0.768 | Val: 0.694\n",
      "Seed: 123, Train: 0.77 | Val: 0.7\n",
      "Seed: 124, Train: 0.768 | Val: 0.69\n",
      "Seed: 125, Train: 0.768 | Val: 0.696\n",
      "Seed: 126, Train: 0.768 | Val: 0.685\n",
      "Seed: 127, Train: 0.769 | Val: 0.703\n",
      "Seed: 128, Train: 0.766 | Val: 0.695\n",
      "Seed: 129, Train: 0.769 | Val: 0.694\n",
      "Seed: 130, Train: 0.769 | Val: 0.695\n",
      "Seed: 131, Train: 0.772 | Val: 0.689\n",
      "Seed: 132, Train: 0.769 | Val: 0.697\n",
      "Seed: 133, Train: 0.768 | Val: 0.7\n",
      "Seed: 134, Train: 0.767 | Val: 0.696\n",
      "Seed: 135, Train: 0.768 | Val: 0.688\n",
      "Seed: 136, Train: 0.768 | Val: 0.689\n",
      "Seed: 137, Train: 0.769 | Val: 0.69\n",
      "Seed: 138, Train: 0.77 | Val: 0.695\n",
      "Seed: 139, Train: 0.771 | Val: 0.695\n",
      "Seed: 140, Train: 0.766 | Val: 0.693\n",
      "Seed: 141, Train: 0.764 | Val: 0.682\n",
      "Seed: 142, Train: 0.768 | Val: 0.687\n",
      "Seed: 143, Train: 0.769 | Val: 0.687\n",
      "Seed: 144, Train: 0.769 | Val: 0.693\n",
      "Seed: 145, Train: 0.77 | Val: 0.694\n",
      "Seed: 146, Train: 0.77 | Val: 0.691\n",
      "Seed: 147, Train: 0.77 | Val: 0.696\n",
      "Seed: 148, Train: 0.766 | Val: 0.699\n",
      "Seed: 149, Train: 0.766 | Val: 0.692\n",
      "Seed: 150, Train: 0.767 | Val: 0.695\n",
      "Seed: 151, Train: 0.77 | Val: 0.692\n",
      "Seed: 152, Train: 0.769 | Val: 0.692\n",
      "Seed: 153, Train: 0.766 | Val: 0.686\n",
      "Seed: 154, Train: 0.769 | Val: 0.693\n",
      "Seed: 155, Train: 0.771 | Val: 0.698\n",
      "Seed: 156, Train: 0.77 | Val: 0.693\n",
      "Seed: 157, Train: 0.769 | Val: 0.693\n",
      "Seed: 158, Train: 0.768 | Val: 0.696\n",
      "Seed: 159, Train: 0.77 | Val: 0.696\n",
      "Seed: 160, Train: 0.769 | Val: 0.69\n",
      "Seed: 161, Train: 0.768 | Val: 0.691\n",
      "Seed: 162, Train: 0.77 | Val: 0.704\n",
      "Seed: 163, Train: 0.77 | Val: 0.692\n",
      "Seed: 164, Train: 0.767 | Val: 0.693\n",
      "Seed: 165, Train: 0.769 | Val: 0.694\n",
      "Seed: 166, Train: 0.77 | Val: 0.692\n",
      "Seed: 167, Train: 0.765 | Val: 0.686\n",
      "Seed: 168, Train: 0.769 | Val: 0.693\n",
      "Seed: 169, Train: 0.767 | Val: 0.687\n",
      "Seed: 170, Train: 0.766 | Val: 0.693\n",
      "Seed: 171, Train: 0.767 | Val: 0.698\n",
      "Seed: 172, Train: 0.765 | Val: 0.699\n",
      "Seed: 173, Train: 0.767 | Val: 0.698\n",
      "Seed: 174, Train: 0.769 | Val: 0.696\n",
      "Seed: 175, Train: 0.768 | Val: 0.697\n",
      "Seed: 176, Train: 0.771 | Val: 0.694\n",
      "Seed: 177, Train: 0.768 | Val: 0.695\n",
      "Seed: 178, Train: 0.772 | Val: 0.691\n",
      "Seed: 179, Train: 0.768 | Val: 0.695\n",
      "Seed: 180, Train: 0.769 | Val: 0.692\n",
      "Seed: 181, Train: 0.768 | Val: 0.698\n",
      "Seed: 182, Train: 0.768 | Val: 0.69\n",
      "Seed: 183, Train: 0.771 | Val: 0.698\n",
      "Seed: 184, Train: 0.77 | Val: 0.694\n",
      "Seed: 185, Train: 0.768 | Val: 0.687\n",
      "Seed: 186, Train: 0.769 | Val: 0.693\n",
      "Seed: 187, Train: 0.766 | Val: 0.697\n",
      "Seed: 188, Train: 0.769 | Val: 0.695\n",
      "Seed: 189, Train: 0.772 | Val: 0.694\n",
      "Seed: 190, Train: 0.77 | Val: 0.692\n",
      "Seed: 191, Train: 0.772 | Val: 0.688\n",
      "Seed: 192, Train: 0.765 | Val: 0.694\n",
      "Seed: 193, Train: 0.768 | Val: 0.689\n",
      "Seed: 194, Train: 0.766 | Val: 0.692\n",
      "Seed: 195, Train: 0.769 | Val: 0.688\n",
      "Seed: 196, Train: 0.769 | Val: 0.685\n",
      "Seed: 197, Train: 0.769 | Val: 0.693\n",
      "Seed: 198, Train: 0.769 | Val: 0.697\n",
      "Seed: 199, Train: 0.772 | Val: 0.694\n",
      "Best is seed 162 with train AUC 0.77 and test AUC 0.704 \n"
     ]
    }
   ],
   "source": [
    "# 0.68, seed 58\n",
    "train_scores_dict = {}\n",
    "test_scores_dict = {}\n",
    "\n",
    "for seed in range(100,200):\n",
    "    X, y = load_preprocess(path = '../input/calcifications.csv',\n",
    "                                                             seed=seed,\n",
    "                                                             apply_ohe = True,\n",
    "                                                             scale=False)\n",
    "    \n",
    "    cv = RepeatedStratifiedKFold(n_repeats=10, n_splits=10, random_state=seed)\n",
    "    \n",
    "    lgbm = LGBMClassifier(objective=\"binary\",\n",
    "                        metric=\"auc\",\n",
    "                        boosting_type=\"gbdt\",\n",
    "                        seed=seed,\n",
    "                        #enable_categorical=True,\n",
    "                        feature_pre_filter=False,\n",
    "                        force_row_wise=True,\n",
    "                        deterministic=True,\n",
    "                        learning_rate=0.05,\n",
    "                        is_unbalance=True,\n",
    "                        verbosity=-1,\n",
    "                        reg_alpha=10,\n",
    "                        reg_lambda=10,\n",
    "                        max_depth=10,\n",
    "                        n_estimators=100,\n",
    "                        colsample_bytree=0.7,                          \n",
    "                        n_jobs=1)\n",
    "    \n",
    "    #mlp_copy = clone(mlp)\n",
    "    #mlp_copy.predict = mlp_copy.predict_proba\n",
    "    \n",
    "    scores = cross_validate(lgbm, \n",
    "                         X, y, \n",
    "                         scoring=\"roc_auc\", \n",
    "                         cv=cv,\n",
    "                         return_train_score=True,\n",
    "                         n_jobs=-1)\n",
    "    \n",
    "    curr_auc_train = np.median(scores[\"train_score\"]).round(3)\n",
    "    curr_auc_test = np.median(scores[\"test_score\"]).round(3)    \n",
    "    train_scores_dict[seed] = curr_auc_train\n",
    "    test_scores_dict[seed] = curr_auc_test\n",
    "    \n",
    "    print(\"Seed: {}, Train: {} | Val: {}\".format(seed, curr_auc_train, curr_auc_test))\n",
    "\n",
    "best_seed = max(test_scores_dict, key=test_scores_dict.get)\n",
    "print(\"Best is seed {} with train AUC {} and test AUC {} \".format(best_seed, \n",
    "                                                                  train_scores_dict[best_seed],\n",
    "                                                                  test_scores_dict[best_seed]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6e99f5e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(colsample_bytree=0.6, deterministic=True,\n",
       "               feature_pre_filter=False, force_row_wise=True,\n",
       "               importance_type='gain', is_unbalance=True, learning_rate=0.05,\n",
       "               max_depth=10, metric='auc', n_jobs=1, objective='binary',\n",
       "               reg_alpha=7, reg_lambda=7, seed=123, verbosity=-1)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm = LGBMClassifier(objective=\"binary\",\n",
    "                        metric=\"auc\",\n",
    "                        boosting_type=\"gbdt\",\n",
    "                        seed=123,\n",
    "                        #enable_categorical=True,\n",
    "                        feature_pre_filter=False,\n",
    "                        force_row_wise=True,\n",
    "                        deterministic=True,\n",
    "                        learning_rate=0.05,\n",
    "                        is_unbalance=True,\n",
    "                        verbosity=-1,\n",
    "                        reg_alpha=7,\n",
    "                        reg_lambda=7,\n",
    "                        max_depth=10,\n",
    "                        n_estimators=100,\n",
    "                        colsample_bytree=0.6,    \n",
    "                       importance_type=\"gain\",\n",
    "                        n_jobs=1)\n",
    "\n",
    "lgbm.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "abe4d47d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('age', 412.5417698306857),\n",
       " ('calcification_right_Type6', 289.98373451828957),\n",
       " ('hyperlipidemia', 106.55086836125702),\n",
       " ('at_least_4', 104.61912578344345),\n",
       " ('stenosis_left', 102.97196879948024),\n",
       " ('stenosis_right', 81.92115885869134),\n",
       " ('calcification_right_Type5', 81.78353455662727),\n",
       " ('calcification_right_Type1', 76.65921981073916),\n",
       " ('calcification_right_Type2', 76.32601677571074),\n",
       " ('calcification_left_Type6', 75.60454034805298),\n",
       " ('calcification_right_Type3', 64.74391509592533),\n",
       " ('calcification_left_Type1', 38.613770335912704),\n",
       " ('smoker_status', 34.79370028366975),\n",
       " ('calcification_right_Type4', 34.551454305648804),\n",
       " ('prs', 29.48992145061493),\n",
       " ('calcification_left_Type5', 12.497104823589325),\n",
       " ('calcification_left_Type4', 6.985429883003235),\n",
       " ('calcification_left_Type2', 4.111389687284827),\n",
       " ('gender', 3.524590525776148),\n",
       " ('hypertension', 2.9609769731760025),\n",
       " ('TIA', 0.0),\n",
       " ('cad', 0.0),\n",
       " ('diabetes', 0.0),\n",
       " ('calcification', 0.0),\n",
       " ('calcification_left_Type3', 0.0)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = X.columns.tolist()\n",
    "vals = lgbm.feature_importances_\n",
    "\n",
    "imps = zip(feats, vals)\n",
    "sorted(imps, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dbc508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8fd512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.integration import OptunaSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import brier_score_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35cf15d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=235)\n",
    "outer_cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=235)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "759238e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _brier_loss(y_true, y_pred):\n",
    "    loss = brier_score_loss(y_true, y_pred)\n",
    "    return \"brier_loss\", -1 * loss, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "811da700",
   "metadata": {},
   "outputs": [],
   "source": [
    "est = LGBMClassifier(objective=\"binary\",\n",
    "                        metric=\"None\",\n",
    "                        boosting_type=\"gbdt\",\n",
    "                        seed=235,\n",
    "                        feature_pre_filter=False,\n",
    "                        force_row_wise=True,\n",
    "                        deterministic=True,\n",
    "                        learning_rate=0.05,\n",
    "                        is_unbalance=True,\n",
    "                        verbosity=-1,\n",
    "                        reg_alpha=10,\n",
    "                        reg_lambda=10,\n",
    "                        max_depth=10,\n",
    "                        n_estimators=100,\n",
    "                        colsample_bytree=0.7,                          \n",
    "                        n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d77e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "            \"reg_alpha\": optuna.distributions.UniformDistribution(6.0, 10.0),\n",
    "            \"reg_lambda\": optuna.distributions.UniformDistribution(6.0, 10.0),\n",
    "            \"num_leaves\": optuna.distributions.IntUniformDistribution(2, 15),\n",
    "            \"colsample_bytree\": optuna.distributions.UniformDistribution(0.6, 0.9),\n",
    "            \"max_depth\": optuna.distributions.IntUniformDistribution(8, 12),\n",
    "            \"n_estimators\": optuna.distributions.IntUniformDistribution(120, 200),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d26e8899",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1p/1ck1ggqs5jg_wdfpm41gx6dr0000gn/T/ipykernel_37383/1361805098.py:1: ExperimentalWarning: OptunaSearchCV is experimental (supported from v0.17.0). The interface can change in the future.\n",
      "  opt_search = OptunaSearchCV(estimator=est,\n",
      "\u001b[32m[I 2022-04-28 15:47:52,020]\u001b[0m A new study created in memory with name: no-name-70ae7ca1-2771-4b4a-adaa-8b515154bc7f\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:52,021]\u001b[0m Searching the best hyperparameters using 711 samples...\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:52,101]\u001b[0m Trial 0 finished with value: -0.222063434745692 and parameters: {'reg_alpha': 6.204519696112449, 'reg_lambda': 6.339273611318688, 'num_leaves': 15, 'colsample_bytree': 0.8022694283947998, 'max_depth': 9, 'n_estimators': 176}. Best is trial 0 with value: -0.222063434745692.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:52,184]\u001b[0m Trial 1 finished with value: -0.22439022312527226 and parameters: {'reg_alpha': 9.64479834174737, 'reg_lambda': 6.000256887693302, 'num_leaves': 13, 'colsample_bytree': 0.7775431936398253, 'max_depth': 10, 'n_estimators': 182}. Best is trial 0 with value: -0.222063434745692.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:52,276]\u001b[0m Trial 2 finished with value: -0.22212094216132333 and parameters: {'reg_alpha': 6.196777448245388, 'reg_lambda': 6.015666646740372, 'num_leaves': 13, 'colsample_bytree': 0.7635738718795501, 'max_depth': 12, 'n_estimators': 156}. Best is trial 0 with value: -0.222063434745692.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:52,388]\u001b[0m Trial 3 finished with value: -0.22323253967396198 and parameters: {'reg_alpha': 7.542502066840683, 'reg_lambda': 9.079589631471556, 'num_leaves': 14, 'colsample_bytree': 0.6349198416214671, 'max_depth': 8, 'n_estimators': 181}. Best is trial 0 with value: -0.222063434745692.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:52,494]\u001b[0m Trial 4 finished with value: -0.22449981988739448 and parameters: {'reg_alpha': 8.906839216973445, 'reg_lambda': 9.364284033741777, 'num_leaves': 11, 'colsample_bytree': 0.6384791654135209, 'max_depth': 12, 'n_estimators': 191}. Best is trial 0 with value: -0.222063434745692.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:52,585]\u001b[0m Trial 5 finished with value: -0.22436770485907967 and parameters: {'reg_alpha': 9.330043611674574, 'reg_lambda': 7.104954501816131, 'num_leaves': 8, 'colsample_bytree': 0.6656062229796421, 'max_depth': 12, 'n_estimators': 160}. Best is trial 0 with value: -0.222063434745692.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:52,667]\u001b[0m Trial 6 finished with value: -0.22428489439186242 and parameters: {'reg_alpha': 9.75228077143339, 'reg_lambda': 9.556125404787046, 'num_leaves': 15, 'colsample_bytree': 0.884276401679743, 'max_depth': 9, 'n_estimators': 169}. Best is trial 0 with value: -0.222063434745692.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:52,766]\u001b[0m Trial 7 finished with value: -0.22394053806212097 and parameters: {'reg_alpha': 8.543387876659587, 'reg_lambda': 9.512968184235525, 'num_leaves': 15, 'colsample_bytree': 0.7952041326293483, 'max_depth': 8, 'n_estimators': 152}. Best is trial 0 with value: -0.222063434745692.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:52,848]\u001b[0m Trial 8 finished with value: -0.2229417045780821 and parameters: {'reg_alpha': 6.494397747535711, 'reg_lambda': 7.574208180774116, 'num_leaves': 13, 'colsample_bytree': 0.7136495859952233, 'max_depth': 8, 'n_estimators': 129}. Best is trial 0 with value: -0.222063434745692.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:52,920]\u001b[0m Trial 9 finished with value: -0.2237253017554332 and parameters: {'reg_alpha': 9.236622832922126, 'reg_lambda': 7.629127560849012, 'num_leaves': 13, 'colsample_bytree': 0.8012442039932435, 'max_depth': 12, 'n_estimators': 176}. Best is trial 0 with value: -0.222063434745692.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:52,923]\u001b[0m Finished hyperparemeter search!\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:52,925]\u001b[0m Refitting the estimator using 711 samples...\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:52,950]\u001b[0m Finished refitting! (elapsed time: 0.020 sec.)\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:52,959]\u001b[0m A new study created in memory with name: no-name-ec58e15c-db70-423d-ba96-cf789a836e35\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:52,962]\u001b[0m Searching the best hyperparameters using 711 samples...\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:53,039]\u001b[0m Trial 0 finished with value: -0.22152174303477504 and parameters: {'reg_alpha': 6.204519696112449, 'reg_lambda': 6.339273611318688, 'num_leaves': 15, 'colsample_bytree': 0.8022694283947998, 'max_depth': 9, 'n_estimators': 176}. Best is trial 0 with value: -0.22152174303477504.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:53,108]\u001b[0m Trial 1 finished with value: -0.2208298612462542 and parameters: {'reg_alpha': 9.64479834174737, 'reg_lambda': 6.000256887693302, 'num_leaves': 13, 'colsample_bytree': 0.7775431936398253, 'max_depth': 10, 'n_estimators': 182}. Best is trial 1 with value: -0.2208298612462542.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:53,200]\u001b[0m Trial 2 finished with value: -0.22082030070010536 and parameters: {'reg_alpha': 6.196777448245388, 'reg_lambda': 6.015666646740372, 'num_leaves': 13, 'colsample_bytree': 0.7635738718795501, 'max_depth': 12, 'n_estimators': 156}. Best is trial 2 with value: -0.22082030070010536.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:53,324]\u001b[0m Trial 3 finished with value: -0.22172434490050913 and parameters: {'reg_alpha': 7.542502066840683, 'reg_lambda': 9.079589631471556, 'num_leaves': 14, 'colsample_bytree': 0.6349198416214671, 'max_depth': 8, 'n_estimators': 181}. Best is trial 2 with value: -0.22082030070010536.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:53,423]\u001b[0m Trial 4 finished with value: -0.2213271155720947 and parameters: {'reg_alpha': 8.906839216973445, 'reg_lambda': 9.364284033741777, 'num_leaves': 11, 'colsample_bytree': 0.6384791654135209, 'max_depth': 12, 'n_estimators': 191}. Best is trial 2 with value: -0.22082030070010536.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:53,494]\u001b[0m Trial 5 finished with value: -0.22164936059021206 and parameters: {'reg_alpha': 9.330043611674574, 'reg_lambda': 7.104954501816131, 'num_leaves': 8, 'colsample_bytree': 0.6656062229796421, 'max_depth': 12, 'n_estimators': 160}. Best is trial 2 with value: -0.22082030070010536.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:53,556]\u001b[0m Trial 6 finished with value: -0.22107183593019836 and parameters: {'reg_alpha': 9.75228077143339, 'reg_lambda': 9.556125404787046, 'num_leaves': 15, 'colsample_bytree': 0.884276401679743, 'max_depth': 9, 'n_estimators': 169}. Best is trial 2 with value: -0.22082030070010536.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:53,639]\u001b[0m Trial 7 finished with value: -0.22042471239532224 and parameters: {'reg_alpha': 8.543387876659587, 'reg_lambda': 9.512968184235525, 'num_leaves': 15, 'colsample_bytree': 0.7952041326293483, 'max_depth': 8, 'n_estimators': 152}. Best is trial 7 with value: -0.22042471239532224.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:53,729]\u001b[0m Trial 8 finished with value: -0.2213978822788291 and parameters: {'reg_alpha': 6.494397747535711, 'reg_lambda': 7.574208180774116, 'num_leaves': 13, 'colsample_bytree': 0.7136495859952233, 'max_depth': 8, 'n_estimators': 129}. Best is trial 7 with value: -0.22042471239532224.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:53,800]\u001b[0m Trial 9 finished with value: -0.22062630876184858 and parameters: {'reg_alpha': 9.236622832922126, 'reg_lambda': 7.629127560849012, 'num_leaves': 13, 'colsample_bytree': 0.8012442039932435, 'max_depth': 12, 'n_estimators': 176}. Best is trial 7 with value: -0.22042471239532224.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:53,801]\u001b[0m Finished hyperparemeter search!\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:53,804]\u001b[0m Refitting the estimator using 711 samples...\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:53,823]\u001b[0m Finished refitting! (elapsed time: 0.014 sec.)\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:53,830]\u001b[0m A new study created in memory with name: no-name-c0c34a48-d865-4729-9547-cbd46bf3763a\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:53,834]\u001b[0m Searching the best hyperparameters using 711 samples...\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:53,903]\u001b[0m Trial 0 finished with value: -0.22014195842061265 and parameters: {'reg_alpha': 6.204519696112449, 'reg_lambda': 6.339273611318688, 'num_leaves': 15, 'colsample_bytree': 0.8022694283947998, 'max_depth': 9, 'n_estimators': 176}. Best is trial 0 with value: -0.22014195842061265.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:53,989]\u001b[0m Trial 1 finished with value: -0.2209914604718047 and parameters: {'reg_alpha': 9.64479834174737, 'reg_lambda': 6.000256887693302, 'num_leaves': 13, 'colsample_bytree': 0.7775431936398253, 'max_depth': 10, 'n_estimators': 182}. Best is trial 0 with value: -0.22014195842061265.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-28 15:47:54,081]\u001b[0m Trial 2 finished with value: -0.21992497742685796 and parameters: {'reg_alpha': 6.196777448245388, 'reg_lambda': 6.015666646740372, 'num_leaves': 13, 'colsample_bytree': 0.7635738718795501, 'max_depth': 12, 'n_estimators': 156}. Best is trial 2 with value: -0.21992497742685796.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:54,182]\u001b[0m Trial 3 finished with value: -0.2203720304314191 and parameters: {'reg_alpha': 7.542502066840683, 'reg_lambda': 9.079589631471556, 'num_leaves': 14, 'colsample_bytree': 0.6349198416214671, 'max_depth': 8, 'n_estimators': 181}. Best is trial 2 with value: -0.21992497742685796.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:54,269]\u001b[0m Trial 4 finished with value: -0.2212206681041534 and parameters: {'reg_alpha': 8.906839216973445, 'reg_lambda': 9.364284033741777, 'num_leaves': 11, 'colsample_bytree': 0.6384791654135209, 'max_depth': 12, 'n_estimators': 191}. Best is trial 2 with value: -0.21992497742685796.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:54,358]\u001b[0m Trial 5 finished with value: -0.22099282320955482 and parameters: {'reg_alpha': 9.330043611674574, 'reg_lambda': 7.104954501816131, 'num_leaves': 8, 'colsample_bytree': 0.6656062229796421, 'max_depth': 12, 'n_estimators': 160}. Best is trial 2 with value: -0.21992497742685796.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:54,444]\u001b[0m Trial 6 finished with value: -0.220810786274881 and parameters: {'reg_alpha': 9.75228077143339, 'reg_lambda': 9.556125404787046, 'num_leaves': 15, 'colsample_bytree': 0.884276401679743, 'max_depth': 9, 'n_estimators': 169}. Best is trial 2 with value: -0.21992497742685796.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:54,518]\u001b[0m Trial 7 finished with value: -0.22035435698526246 and parameters: {'reg_alpha': 8.543387876659587, 'reg_lambda': 9.512968184235525, 'num_leaves': 15, 'colsample_bytree': 0.7952041326293483, 'max_depth': 8, 'n_estimators': 152}. Best is trial 2 with value: -0.21992497742685796.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:54,598]\u001b[0m Trial 8 finished with value: -0.2201437489543352 and parameters: {'reg_alpha': 6.494397747535711, 'reg_lambda': 7.574208180774116, 'num_leaves': 13, 'colsample_bytree': 0.7136495859952233, 'max_depth': 8, 'n_estimators': 129}. Best is trial 2 with value: -0.21992497742685796.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:54,665]\u001b[0m Trial 9 finished with value: -0.22075687375053596 and parameters: {'reg_alpha': 9.236622832922126, 'reg_lambda': 7.629127560849012, 'num_leaves': 13, 'colsample_bytree': 0.8012442039932435, 'max_depth': 12, 'n_estimators': 176}. Best is trial 2 with value: -0.21992497742685796.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:54,667]\u001b[0m Finished hyperparemeter search!\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:54,668]\u001b[0m Refitting the estimator using 711 samples...\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:54,686]\u001b[0m Finished refitting! (elapsed time: 0.014 sec.)\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:54,693]\u001b[0m A new study created in memory with name: no-name-03088bb1-745c-4cff-bf49-fbd6315aef93\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:54,694]\u001b[0m Searching the best hyperparameters using 711 samples...\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:54,787]\u001b[0m Trial 0 finished with value: -0.2141533100625181 and parameters: {'reg_alpha': 6.204519696112449, 'reg_lambda': 6.339273611318688, 'num_leaves': 15, 'colsample_bytree': 0.8022694283947998, 'max_depth': 9, 'n_estimators': 176}. Best is trial 0 with value: -0.2141533100625181.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:54,853]\u001b[0m Trial 1 finished with value: -0.21761297002531754 and parameters: {'reg_alpha': 9.64479834174737, 'reg_lambda': 6.000256887693302, 'num_leaves': 13, 'colsample_bytree': 0.7775431936398253, 'max_depth': 10, 'n_estimators': 182}. Best is trial 0 with value: -0.2141533100625181.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:54,934]\u001b[0m Trial 2 finished with value: -0.2145570236638839 and parameters: {'reg_alpha': 6.196777448245388, 'reg_lambda': 6.015666646740372, 'num_leaves': 13, 'colsample_bytree': 0.7635738718795501, 'max_depth': 12, 'n_estimators': 156}. Best is trial 0 with value: -0.2141533100625181.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:55,029]\u001b[0m Trial 3 finished with value: -0.21632746570072064 and parameters: {'reg_alpha': 7.542502066840683, 'reg_lambda': 9.079589631471556, 'num_leaves': 14, 'colsample_bytree': 0.6349198416214671, 'max_depth': 8, 'n_estimators': 181}. Best is trial 0 with value: -0.2141533100625181.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:55,114]\u001b[0m Trial 4 finished with value: -0.21791848565884347 and parameters: {'reg_alpha': 8.906839216973445, 'reg_lambda': 9.364284033741777, 'num_leaves': 11, 'colsample_bytree': 0.6384791654135209, 'max_depth': 12, 'n_estimators': 191}. Best is trial 0 with value: -0.2141533100625181.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:55,205]\u001b[0m Trial 5 finished with value: -0.2178934087290907 and parameters: {'reg_alpha': 9.330043611674574, 'reg_lambda': 7.104954501816131, 'num_leaves': 8, 'colsample_bytree': 0.6656062229796421, 'max_depth': 12, 'n_estimators': 160}. Best is trial 0 with value: -0.2141533100625181.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:55,267]\u001b[0m Trial 6 finished with value: -0.21751615565902763 and parameters: {'reg_alpha': 9.75228077143339, 'reg_lambda': 9.556125404787046, 'num_leaves': 15, 'colsample_bytree': 0.884276401679743, 'max_depth': 9, 'n_estimators': 169}. Best is trial 0 with value: -0.2141533100625181.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:55,361]\u001b[0m Trial 7 finished with value: -0.21721594125801005 and parameters: {'reg_alpha': 8.543387876659587, 'reg_lambda': 9.512968184235525, 'num_leaves': 15, 'colsample_bytree': 0.7952041326293483, 'max_depth': 8, 'n_estimators': 152}. Best is trial 0 with value: -0.2141533100625181.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:55,445]\u001b[0m Trial 8 finished with value: -0.21521348715731134 and parameters: {'reg_alpha': 6.494397747535711, 'reg_lambda': 7.574208180774116, 'num_leaves': 13, 'colsample_bytree': 0.7136495859952233, 'max_depth': 8, 'n_estimators': 129}. Best is trial 0 with value: -0.2141533100625181.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:55,512]\u001b[0m Trial 9 finished with value: -0.21757618792383657 and parameters: {'reg_alpha': 9.236622832922126, 'reg_lambda': 7.629127560849012, 'num_leaves': 13, 'colsample_bytree': 0.8012442039932435, 'max_depth': 12, 'n_estimators': 176}. Best is trial 0 with value: -0.2141533100625181.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:55,521]\u001b[0m Finished hyperparemeter search!\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:55,529]\u001b[0m Refitting the estimator using 711 samples...\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:55,546]\u001b[0m Finished refitting! (elapsed time: 0.015 sec.)\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:55,550]\u001b[0m A new study created in memory with name: no-name-47c747e9-70ec-4134-8578-0e9cf5af1196\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:55,551]\u001b[0m Searching the best hyperparameters using 711 samples...\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:55,625]\u001b[0m Trial 0 finished with value: -0.2216791171212297 and parameters: {'reg_alpha': 6.204519696112449, 'reg_lambda': 6.339273611318688, 'num_leaves': 15, 'colsample_bytree': 0.8022694283947998, 'max_depth': 9, 'n_estimators': 176}. Best is trial 0 with value: -0.2216791171212297.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:55,714]\u001b[0m Trial 1 finished with value: -0.22182626975186018 and parameters: {'reg_alpha': 9.64479834174737, 'reg_lambda': 6.000256887693302, 'num_leaves': 13, 'colsample_bytree': 0.7775431936398253, 'max_depth': 10, 'n_estimators': 182}. Best is trial 0 with value: -0.2216791171212297.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:55,789]\u001b[0m Trial 2 finished with value: -0.2213783140492495 and parameters: {'reg_alpha': 6.196777448245388, 'reg_lambda': 6.015666646740372, 'num_leaves': 13, 'colsample_bytree': 0.7635738718795501, 'max_depth': 12, 'n_estimators': 156}. Best is trial 2 with value: -0.2213783140492495.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:55,874]\u001b[0m Trial 3 finished with value: -0.2214778098313827 and parameters: {'reg_alpha': 7.542502066840683, 'reg_lambda': 9.079589631471556, 'num_leaves': 14, 'colsample_bytree': 0.6349198416214671, 'max_depth': 8, 'n_estimators': 181}. Best is trial 2 with value: -0.2213783140492495.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:55,977]\u001b[0m Trial 4 finished with value: -0.22180875054077834 and parameters: {'reg_alpha': 8.906839216973445, 'reg_lambda': 9.364284033741777, 'num_leaves': 11, 'colsample_bytree': 0.6384791654135209, 'max_depth': 12, 'n_estimators': 191}. Best is trial 2 with value: -0.2213783140492495.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-28 15:47:56,068]\u001b[0m Trial 5 finished with value: -0.2221723065021842 and parameters: {'reg_alpha': 9.330043611674574, 'reg_lambda': 7.104954501816131, 'num_leaves': 8, 'colsample_bytree': 0.6656062229796421, 'max_depth': 12, 'n_estimators': 160}. Best is trial 2 with value: -0.2213783140492495.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:56,136]\u001b[0m Trial 6 finished with value: -0.22152122691597048 and parameters: {'reg_alpha': 9.75228077143339, 'reg_lambda': 9.556125404787046, 'num_leaves': 15, 'colsample_bytree': 0.884276401679743, 'max_depth': 9, 'n_estimators': 169}. Best is trial 2 with value: -0.2213783140492495.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:56,198]\u001b[0m Trial 7 finished with value: -0.22142142635994566 and parameters: {'reg_alpha': 8.543387876659587, 'reg_lambda': 9.512968184235525, 'num_leaves': 15, 'colsample_bytree': 0.7952041326293483, 'max_depth': 8, 'n_estimators': 152}. Best is trial 2 with value: -0.2213783140492495.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:56,294]\u001b[0m Trial 8 finished with value: -0.22132800332745614 and parameters: {'reg_alpha': 6.494397747535711, 'reg_lambda': 7.574208180774116, 'num_leaves': 13, 'colsample_bytree': 0.7136495859952233, 'max_depth': 8, 'n_estimators': 129}. Best is trial 8 with value: -0.22132800332745614.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:56,374]\u001b[0m Trial 9 finished with value: -0.22157430474566406 and parameters: {'reg_alpha': 9.236622832922126, 'reg_lambda': 7.629127560849012, 'num_leaves': 13, 'colsample_bytree': 0.8012442039932435, 'max_depth': 12, 'n_estimators': 176}. Best is trial 8 with value: -0.22132800332745614.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:56,380]\u001b[0m Finished hyperparemeter search!\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:56,384]\u001b[0m Refitting the estimator using 711 samples...\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:56,398]\u001b[0m Finished refitting! (elapsed time: 0.014 sec.)\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:56,402]\u001b[0m A new study created in memory with name: no-name-dda18664-7719-4325-9242-cde742031f5c\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:56,403]\u001b[0m Searching the best hyperparameters using 711 samples...\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:56,477]\u001b[0m Trial 0 finished with value: -0.2114130081665598 and parameters: {'reg_alpha': 6.204519696112449, 'reg_lambda': 6.339273611318688, 'num_leaves': 15, 'colsample_bytree': 0.8022694283947998, 'max_depth': 9, 'n_estimators': 176}. Best is trial 0 with value: -0.2114130081665598.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:56,560]\u001b[0m Trial 1 finished with value: -0.21416886455650302 and parameters: {'reg_alpha': 9.64479834174737, 'reg_lambda': 6.000256887693302, 'num_leaves': 13, 'colsample_bytree': 0.7775431936398253, 'max_depth': 10, 'n_estimators': 182}. Best is trial 0 with value: -0.2114130081665598.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:56,643]\u001b[0m Trial 2 finished with value: -0.2108609232500917 and parameters: {'reg_alpha': 6.196777448245388, 'reg_lambda': 6.015666646740372, 'num_leaves': 13, 'colsample_bytree': 0.7635738718795501, 'max_depth': 12, 'n_estimators': 156}. Best is trial 2 with value: -0.2108609232500917.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:56,737]\u001b[0m Trial 3 finished with value: -0.21429091405872464 and parameters: {'reg_alpha': 7.542502066840683, 'reg_lambda': 9.079589631471556, 'num_leaves': 14, 'colsample_bytree': 0.6349198416214671, 'max_depth': 8, 'n_estimators': 181}. Best is trial 2 with value: -0.2108609232500917.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:56,829]\u001b[0m Trial 4 finished with value: -0.21575460554597675 and parameters: {'reg_alpha': 8.906839216973445, 'reg_lambda': 9.364284033741777, 'num_leaves': 11, 'colsample_bytree': 0.6384791654135209, 'max_depth': 12, 'n_estimators': 191}. Best is trial 2 with value: -0.2108609232500917.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:56,926]\u001b[0m Trial 5 finished with value: -0.2165674070827764 and parameters: {'reg_alpha': 9.330043611674574, 'reg_lambda': 7.104954501816131, 'num_leaves': 8, 'colsample_bytree': 0.6656062229796421, 'max_depth': 12, 'n_estimators': 160}. Best is trial 2 with value: -0.2108609232500917.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:57,008]\u001b[0m Trial 6 finished with value: -0.21465366112713716 and parameters: {'reg_alpha': 9.75228077143339, 'reg_lambda': 9.556125404787046, 'num_leaves': 15, 'colsample_bytree': 0.884276401679743, 'max_depth': 9, 'n_estimators': 169}. Best is trial 2 with value: -0.2108609232500917.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:57,101]\u001b[0m Trial 7 finished with value: -0.21328995843003024 and parameters: {'reg_alpha': 8.543387876659587, 'reg_lambda': 9.512968184235525, 'num_leaves': 15, 'colsample_bytree': 0.7952041326293483, 'max_depth': 8, 'n_estimators': 152}. Best is trial 2 with value: -0.2108609232500917.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:57,192]\u001b[0m Trial 8 finished with value: -0.21194751719620294 and parameters: {'reg_alpha': 6.494397747535711, 'reg_lambda': 7.574208180774116, 'num_leaves': 13, 'colsample_bytree': 0.7136495859952233, 'max_depth': 8, 'n_estimators': 129}. Best is trial 2 with value: -0.2108609232500917.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:57,261]\u001b[0m Trial 9 finished with value: -0.21426468660865433 and parameters: {'reg_alpha': 9.236622832922126, 'reg_lambda': 7.629127560849012, 'num_leaves': 13, 'colsample_bytree': 0.8012442039932435, 'max_depth': 12, 'n_estimators': 176}. Best is trial 2 with value: -0.2108609232500917.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:57,263]\u001b[0m Finished hyperparemeter search!\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:57,273]\u001b[0m Refitting the estimator using 711 samples...\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:57,368]\u001b[0m Finished refitting! (elapsed time: 0.066 sec.)\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:57,387]\u001b[0m A new study created in memory with name: no-name-ee152f7c-dd60-40e2-9582-205b7c64104a\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:57,398]\u001b[0m Searching the best hyperparameters using 711 samples...\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:57,492]\u001b[0m Trial 0 finished with value: -0.22068836514119247 and parameters: {'reg_alpha': 6.204519696112449, 'reg_lambda': 6.339273611318688, 'num_leaves': 15, 'colsample_bytree': 0.8022694283947998, 'max_depth': 9, 'n_estimators': 176}. Best is trial 0 with value: -0.22068836514119247.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:57,551]\u001b[0m Trial 1 finished with value: -0.22309074819366942 and parameters: {'reg_alpha': 9.64479834174737, 'reg_lambda': 6.000256887693302, 'num_leaves': 13, 'colsample_bytree': 0.7775431936398253, 'max_depth': 10, 'n_estimators': 182}. Best is trial 0 with value: -0.22068836514119247.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:57,624]\u001b[0m Trial 2 finished with value: -0.22053354554287963 and parameters: {'reg_alpha': 6.196777448245388, 'reg_lambda': 6.015666646740372, 'num_leaves': 13, 'colsample_bytree': 0.7635738718795501, 'max_depth': 12, 'n_estimators': 156}. Best is trial 2 with value: -0.22053354554287963.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:57,739]\u001b[0m Trial 3 finished with value: -0.22141259662233814 and parameters: {'reg_alpha': 7.542502066840683, 'reg_lambda': 9.079589631471556, 'num_leaves': 14, 'colsample_bytree': 0.6349198416214671, 'max_depth': 8, 'n_estimators': 181}. Best is trial 2 with value: -0.22053354554287963.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:57,841]\u001b[0m Trial 4 finished with value: -0.22259422805929016 and parameters: {'reg_alpha': 8.906839216973445, 'reg_lambda': 9.364284033741777, 'num_leaves': 11, 'colsample_bytree': 0.6384791654135209, 'max_depth': 12, 'n_estimators': 191}. Best is trial 2 with value: -0.22053354554287963.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:57,931]\u001b[0m Trial 5 finished with value: -0.22270391478988394 and parameters: {'reg_alpha': 9.330043611674574, 'reg_lambda': 7.104954501816131, 'num_leaves': 8, 'colsample_bytree': 0.6656062229796421, 'max_depth': 12, 'n_estimators': 160}. Best is trial 2 with value: -0.22053354554287963.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:57,985]\u001b[0m Trial 6 finished with value: -0.22243807035035354 and parameters: {'reg_alpha': 9.75228077143339, 'reg_lambda': 9.556125404787046, 'num_leaves': 15, 'colsample_bytree': 0.884276401679743, 'max_depth': 9, 'n_estimators': 169}. Best is trial 2 with value: -0.22053354554287963.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:58,057]\u001b[0m Trial 7 finished with value: -0.2215740927274187 and parameters: {'reg_alpha': 8.543387876659587, 'reg_lambda': 9.512968184235525, 'num_leaves': 15, 'colsample_bytree': 0.7952041326293483, 'max_depth': 8, 'n_estimators': 152}. Best is trial 2 with value: -0.22053354554287963.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-28 15:47:58,128]\u001b[0m Trial 8 finished with value: -0.21991676185691253 and parameters: {'reg_alpha': 6.494397747535711, 'reg_lambda': 7.574208180774116, 'num_leaves': 13, 'colsample_bytree': 0.7136495859952233, 'max_depth': 8, 'n_estimators': 129}. Best is trial 8 with value: -0.21991676185691253.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:58,230]\u001b[0m Trial 9 finished with value: -0.22259259917812613 and parameters: {'reg_alpha': 9.236622832922126, 'reg_lambda': 7.629127560849012, 'num_leaves': 13, 'colsample_bytree': 0.8012442039932435, 'max_depth': 12, 'n_estimators': 176}. Best is trial 8 with value: -0.21991676185691253.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:58,232]\u001b[0m Finished hyperparemeter search!\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:58,235]\u001b[0m Refitting the estimator using 711 samples...\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:58,262]\u001b[0m Finished refitting! (elapsed time: 0.013 sec.)\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:58,268]\u001b[0m A new study created in memory with name: no-name-2c01199b-20a9-4ba7-b313-3506d85db90a\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:58,270]\u001b[0m Searching the best hyperparameters using 711 samples...\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:58,340]\u001b[0m Trial 0 finished with value: -0.218320511836655 and parameters: {'reg_alpha': 6.204519696112449, 'reg_lambda': 6.339273611318688, 'num_leaves': 15, 'colsample_bytree': 0.8022694283947998, 'max_depth': 9, 'n_estimators': 176}. Best is trial 0 with value: -0.218320511836655.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:58,415]\u001b[0m Trial 1 finished with value: -0.22229747881427625 and parameters: {'reg_alpha': 9.64479834174737, 'reg_lambda': 6.000256887693302, 'num_leaves': 13, 'colsample_bytree': 0.7775431936398253, 'max_depth': 10, 'n_estimators': 182}. Best is trial 0 with value: -0.218320511836655.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:58,504]\u001b[0m Trial 2 finished with value: -0.21856719581600856 and parameters: {'reg_alpha': 6.196777448245388, 'reg_lambda': 6.015666646740372, 'num_leaves': 13, 'colsample_bytree': 0.7635738718795501, 'max_depth': 12, 'n_estimators': 156}. Best is trial 0 with value: -0.218320511836655.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:58,607]\u001b[0m Trial 3 finished with value: -0.22112711936493415 and parameters: {'reg_alpha': 7.542502066840683, 'reg_lambda': 9.079589631471556, 'num_leaves': 14, 'colsample_bytree': 0.6349198416214671, 'max_depth': 8, 'n_estimators': 181}. Best is trial 0 with value: -0.218320511836655.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:58,692]\u001b[0m Trial 4 finished with value: -0.2228537907078886 and parameters: {'reg_alpha': 8.906839216973445, 'reg_lambda': 9.364284033741777, 'num_leaves': 11, 'colsample_bytree': 0.6384791654135209, 'max_depth': 12, 'n_estimators': 191}. Best is trial 0 with value: -0.218320511836655.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:58,777]\u001b[0m Trial 5 finished with value: -0.22317497490221988 and parameters: {'reg_alpha': 9.330043611674574, 'reg_lambda': 7.104954501816131, 'num_leaves': 8, 'colsample_bytree': 0.6656062229796421, 'max_depth': 12, 'n_estimators': 160}. Best is trial 0 with value: -0.218320511836655.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:58,864]\u001b[0m Trial 6 finished with value: -0.2218139268086957 and parameters: {'reg_alpha': 9.75228077143339, 'reg_lambda': 9.556125404787046, 'num_leaves': 15, 'colsample_bytree': 0.884276401679743, 'max_depth': 9, 'n_estimators': 169}. Best is trial 0 with value: -0.218320511836655.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:58,926]\u001b[0m Trial 7 finished with value: -0.2207250614437979 and parameters: {'reg_alpha': 8.543387876659587, 'reg_lambda': 9.512968184235525, 'num_leaves': 15, 'colsample_bytree': 0.7952041326293483, 'max_depth': 8, 'n_estimators': 152}. Best is trial 0 with value: -0.218320511836655.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:59,025]\u001b[0m Trial 8 finished with value: -0.21960954115998063 and parameters: {'reg_alpha': 6.494397747535711, 'reg_lambda': 7.574208180774116, 'num_leaves': 13, 'colsample_bytree': 0.7136495859952233, 'max_depth': 8, 'n_estimators': 129}. Best is trial 0 with value: -0.218320511836655.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:59,097]\u001b[0m Trial 9 finished with value: -0.22181585447211805 and parameters: {'reg_alpha': 9.236622832922126, 'reg_lambda': 7.629127560849012, 'num_leaves': 13, 'colsample_bytree': 0.8012442039932435, 'max_depth': 12, 'n_estimators': 176}. Best is trial 0 with value: -0.218320511836655.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:59,098]\u001b[0m Finished hyperparemeter search!\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:59,106]\u001b[0m Refitting the estimator using 711 samples...\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:59,130]\u001b[0m Finished refitting! (elapsed time: 0.022 sec.)\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:59,135]\u001b[0m A new study created in memory with name: no-name-9ff1af7a-0344-4f4e-9099-aab05aeed106\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:59,137]\u001b[0m Searching the best hyperparameters using 711 samples...\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:59,230]\u001b[0m Trial 0 finished with value: -0.2193883845232724 and parameters: {'reg_alpha': 6.204519696112449, 'reg_lambda': 6.339273611318688, 'num_leaves': 15, 'colsample_bytree': 0.8022694283947998, 'max_depth': 9, 'n_estimators': 176}. Best is trial 0 with value: -0.2193883845232724.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:59,326]\u001b[0m Trial 1 finished with value: -0.2187984650308472 and parameters: {'reg_alpha': 9.64479834174737, 'reg_lambda': 6.000256887693302, 'num_leaves': 13, 'colsample_bytree': 0.7775431936398253, 'max_depth': 10, 'n_estimators': 182}. Best is trial 1 with value: -0.2187984650308472.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:59,425]\u001b[0m Trial 2 finished with value: -0.2189171394607959 and parameters: {'reg_alpha': 6.196777448245388, 'reg_lambda': 6.015666646740372, 'num_leaves': 13, 'colsample_bytree': 0.7635738718795501, 'max_depth': 12, 'n_estimators': 156}. Best is trial 1 with value: -0.2187984650308472.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:59,505]\u001b[0m Trial 3 finished with value: -0.21897140093711492 and parameters: {'reg_alpha': 7.542502066840683, 'reg_lambda': 9.079589631471556, 'num_leaves': 14, 'colsample_bytree': 0.6349198416214671, 'max_depth': 8, 'n_estimators': 181}. Best is trial 1 with value: -0.2187984650308472.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:59,589]\u001b[0m Trial 4 finished with value: -0.2193418506832942 and parameters: {'reg_alpha': 8.906839216973445, 'reg_lambda': 9.364284033741777, 'num_leaves': 11, 'colsample_bytree': 0.6384791654135209, 'max_depth': 12, 'n_estimators': 191}. Best is trial 1 with value: -0.2187984650308472.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:59,669]\u001b[0m Trial 5 finished with value: -0.21942281203249098 and parameters: {'reg_alpha': 9.330043611674574, 'reg_lambda': 7.104954501816131, 'num_leaves': 8, 'colsample_bytree': 0.6656062229796421, 'max_depth': 12, 'n_estimators': 160}. Best is trial 1 with value: -0.2187984650308472.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:59,761]\u001b[0m Trial 6 finished with value: -0.2184476266198699 and parameters: {'reg_alpha': 9.75228077143339, 'reg_lambda': 9.556125404787046, 'num_leaves': 15, 'colsample_bytree': 0.884276401679743, 'max_depth': 9, 'n_estimators': 169}. Best is trial 6 with value: -0.2184476266198699.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:59,828]\u001b[0m Trial 7 finished with value: -0.21847942579129262 and parameters: {'reg_alpha': 8.543387876659587, 'reg_lambda': 9.512968184235525, 'num_leaves': 15, 'colsample_bytree': 0.7952041326293483, 'max_depth': 8, 'n_estimators': 152}. Best is trial 6 with value: -0.2184476266198699.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:59,911]\u001b[0m Trial 8 finished with value: -0.2192041197114445 and parameters: {'reg_alpha': 6.494397747535711, 'reg_lambda': 7.574208180774116, 'num_leaves': 13, 'colsample_bytree': 0.7136495859952233, 'max_depth': 8, 'n_estimators': 129}. Best is trial 6 with value: -0.2184476266198699.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:59,980]\u001b[0m Trial 9 finished with value: -0.21835362517656565 and parameters: {'reg_alpha': 9.236622832922126, 'reg_lambda': 7.629127560849012, 'num_leaves': 13, 'colsample_bytree': 0.8012442039932435, 'max_depth': 12, 'n_estimators': 176}. Best is trial 9 with value: -0.21835362517656565.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:59,983]\u001b[0m Finished hyperparemeter search!\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:47:59,986]\u001b[0m Refitting the estimator using 711 samples...\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:48:00,014]\u001b[0m Finished refitting! (elapsed time: 0.023 sec.)\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:48:00,019]\u001b[0m A new study created in memory with name: no-name-fc99aaf3-e201-46d4-b851-46d3085e147d\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-28 15:48:00,020]\u001b[0m Searching the best hyperparameters using 711 samples...\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:48:00,123]\u001b[0m Trial 0 finished with value: -0.2145530091092267 and parameters: {'reg_alpha': 6.204519696112449, 'reg_lambda': 6.339273611318688, 'num_leaves': 15, 'colsample_bytree': 0.8022694283947998, 'max_depth': 9, 'n_estimators': 176}. Best is trial 0 with value: -0.2145530091092267.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:48:00,194]\u001b[0m Trial 1 finished with value: -0.21802473546119158 and parameters: {'reg_alpha': 9.64479834174737, 'reg_lambda': 6.000256887693302, 'num_leaves': 13, 'colsample_bytree': 0.7775431936398253, 'max_depth': 10, 'n_estimators': 182}. Best is trial 0 with value: -0.2145530091092267.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:48:00,282]\u001b[0m Trial 2 finished with value: -0.2150368710183787 and parameters: {'reg_alpha': 6.196777448245388, 'reg_lambda': 6.015666646740372, 'num_leaves': 13, 'colsample_bytree': 0.7635738718795501, 'max_depth': 12, 'n_estimators': 156}. Best is trial 0 with value: -0.2145530091092267.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:48:00,384]\u001b[0m Trial 3 finished with value: -0.21751279668857845 and parameters: {'reg_alpha': 7.542502066840683, 'reg_lambda': 9.079589631471556, 'num_leaves': 14, 'colsample_bytree': 0.6349198416214671, 'max_depth': 8, 'n_estimators': 181}. Best is trial 0 with value: -0.2145530091092267.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:48:00,462]\u001b[0m Trial 4 finished with value: -0.21877886014531933 and parameters: {'reg_alpha': 8.906839216973445, 'reg_lambda': 9.364284033741777, 'num_leaves': 11, 'colsample_bytree': 0.6384791654135209, 'max_depth': 12, 'n_estimators': 191}. Best is trial 0 with value: -0.2145530091092267.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:48:00,551]\u001b[0m Trial 5 finished with value: -0.21903535781167704 and parameters: {'reg_alpha': 9.330043611674574, 'reg_lambda': 7.104954501816131, 'num_leaves': 8, 'colsample_bytree': 0.6656062229796421, 'max_depth': 12, 'n_estimators': 160}. Best is trial 0 with value: -0.2145530091092267.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:48:00,619]\u001b[0m Trial 6 finished with value: -0.21748876249747315 and parameters: {'reg_alpha': 9.75228077143339, 'reg_lambda': 9.556125404787046, 'num_leaves': 15, 'colsample_bytree': 0.884276401679743, 'max_depth': 9, 'n_estimators': 169}. Best is trial 0 with value: -0.2145530091092267.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:48:00,697]\u001b[0m Trial 7 finished with value: -0.21755070794216236 and parameters: {'reg_alpha': 8.543387876659587, 'reg_lambda': 9.512968184235525, 'num_leaves': 15, 'colsample_bytree': 0.7952041326293483, 'max_depth': 8, 'n_estimators': 152}. Best is trial 0 with value: -0.2145530091092267.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:48:00,799]\u001b[0m Trial 8 finished with value: -0.21551270995699304 and parameters: {'reg_alpha': 6.494397747535711, 'reg_lambda': 7.574208180774116, 'num_leaves': 13, 'colsample_bytree': 0.7136495859952233, 'max_depth': 8, 'n_estimators': 129}. Best is trial 0 with value: -0.2145530091092267.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:48:00,871]\u001b[0m Trial 9 finished with value: -0.217838309693475 and parameters: {'reg_alpha': 9.236622832922126, 'reg_lambda': 7.629127560849012, 'num_leaves': 13, 'colsample_bytree': 0.8012442039932435, 'max_depth': 12, 'n_estimators': 176}. Best is trial 0 with value: -0.2145530091092267.\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:48:00,885]\u001b[0m Finished hyperparemeter search!\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:48:00,889]\u001b[0m Refitting the estimator using 711 samples...\u001b[0m\n",
      "\u001b[32m[I 2022-04-28 15:48:00,913]\u001b[0m Finished refitting! (elapsed time: 0.019 sec.)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "opt_search = OptunaSearchCV(estimator=est,\n",
    "                                    param_distributions=space,\n",
    "                                    cv=inner_cv,\n",
    "                                    n_trials=10,\n",
    "                                    random_state=235,\n",
    "                                    refit=True,\n",
    "                                    scoring=\"neg_brier_score\",\n",
    "                                    verbose=10,\n",
    "                                    n_jobs=1)\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(outer_cv.split(X, y)):\n",
    "    Xtrain, ytrain = X.iloc[train_idx], y[train_idx]\n",
    "    Xtest, ytest = X.loc[test_idx], y[test_idx]\n",
    "    \n",
    "    \"\"\"\n",
    "    if (i%10)+1 == 10:\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    print(\"Iteration {}\".format((i % 10)+1))\n",
    "    \"\"\"\n",
    "    \n",
    "    opt_search.fit(Xtrain, ytrain)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b26282fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.025"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 1\n",
    "i / 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52335f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d91e3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "280a481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../output/predictions/outer_cv_accumulated_preds_all_algorithms.pkl\", \"rb\") as f_r:\n",
    "    obj = pickle.load(f_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f44b454b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm:  LGBM\n",
      "ROCAUC: 0.7139 [0.57 - 0.81]\n",
      "PRAUC: 0.8197 [0.72 - 0.90]\n",
      "Brier: 0.2165 [0.19 - 0.23]\n",
      "Algorithm:  MLP\n",
      "ROCAUC: 0.6551 [0.51 - 0.76]\n",
      "PRAUC: 0.7823 [0.68 - 0.86]\n",
      "Brier: 0.2188 [0.19 - 0.25]\n",
      "Algorithm:  SVC\n",
      "ROCAUC: 0.5060 [0.39 - 0.62]\n",
      "PRAUC: 0.6684 [0.59 - 0.77]\n",
      "Brier: 0.2289 [0.23 - 0.24]\n",
      "Algorithm:  LR\n",
      "ROCAUC: 0.6569 [0.55 - 0.77]\n",
      "PRAUC: 0.7872 [0.71 - 0.86]\n",
      "Brier: 0.2311 [0.20 - 0.26]\n",
      "Algorithm:  KNN\n",
      "ROCAUC: 0.5042 [0.39 - 0.65]\n",
      "PRAUC: 0.6636 [0.59 - 0.74]\n",
      "Brier: 0.2382 [0.22 - 0.26]\n"
     ]
    }
   ],
   "source": [
    "for j, (algo_name, outer_cv_data) in enumerate(obj.items()):\n",
    "    print(\"Algorithm: \", algo_name)\n",
    "    \n",
    "    roc_aucs = []\n",
    "    pr_aucs = []\n",
    "    briers = []\n",
    "    \n",
    "    gts = outer_cv_data['gt']\n",
    "    probas = outer_cv_data['probas']\n",
    "    \n",
    "    for split_idx, (split_gts, split_probas) in enumerate(zip(gts, probas)):\n",
    "        rocauc_val = roc_auc_score(split_gts, split_probas[:, 1])\n",
    "        prauc_val = average_precision_score(split_gts, split_probas[:, 1])\n",
    "        brier_val = brier_score_loss(split_gts, split_probas[:, 1])\n",
    "        \n",
    "        roc_aucs.append(rocauc_val)\n",
    "        pr_aucs.append(prauc_val)\n",
    "        briers.append(brier_val)\n",
    "\n",
    "    rocauc_low, rocauc_med, rocauc_up = np.percentile(roc_aucs, [2.5, 50, 97.5])\n",
    "    prauc_low, prauc_med, prauc_up = np.percentile(pr_aucs, [2.5, 50, 97.5])\n",
    "    brier_low, brier_med, brier_up = np.percentile(briers, [2.5, 50, 97.5])\n",
    "    \n",
    "    print(\"ROCAUC: {:.4f} [{:.2f} - {:.2f}]\".format(rocauc_med, rocauc_low, rocauc_up))\n",
    "    print(\"PRAUC: {:.4f} [{:.2f} - {:.2f}]\".format(prauc_med, prauc_low, prauc_up))\n",
    "    print(\"Brier: {:.4f} [{:.2f} - {:.2f}]\".format(brier_med, brier_low, brier_up))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
