---
title: "ROC and PR curves"
author: "Francesco Pisu"
date: '2022-03-07'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
install.packages('pROC')
install.packages("PRROC")
install.packages("stringr")
install.packages("boot")
install.packages("MLmetrics")
install.packages("ggplot2")
install.packages("yardstick")
install.packages("ggsci")
install.packages("cvms")
install.packages("gridBase")
install.packages("dplyr")
```

```{r}
library("pROC")
library("PRROC")
library("stringr")
library("boot")
library("MLmetrics")
library("ggplot2")
library("yardstick")
library("ggsci")
library("cvms")
library("gridBase")
library("grid")
library("dplyr")
```


# Load predicted probabilities on both training and test sets
```{r}
train = read.csv('../output/results/train_target_proba.csv')
test = read.csv('../output/results/test_target_proba.csv')
```

```{r}
lightblue <- "#b0c4de"
salmon <- "#ff8c69"
```


# ROC curves
```{r}
pdf("../output/plots/roc.pdf", width = 6)
par(pty="s")
test.roc <- pROC::roc(test$target, test$proba, ci=TRUE, plot=TRUE,
    legacy.axes=TRUE, lwd=3, col=salmon)
title("ROC curves", line=2.3)

test.roc.auc <- round(pROC::auc(test.roc), 2)
test.roc.ci <- pROC::ci.auc(test.roc, method="bootstrap")

train.roc <- pROC::roc(train$target, train$proba, ci=TRUE, add=TRUE, plot=TRUE,
    legacy.axes=TRUE, lwd=3, col=lightblue)

train.roc.auc <- round(pROC::auc(train.roc), 2)
train.roc.ci <- pROC::ci.auc(train.roc, method="bootstrap")

roc.train.legend.text <- stringr::str_interp("Train AUC $[.2f]{train.roc.auc} [$[.2f]{train.roc.ci[1]}-$[.2f]{train.roc.ci[3]}]")
roc.test.legend.text <- stringr::str_interp("Test  AUC $[.2f]{test.roc.auc} [$[.2f]{test.roc.ci[1]}-$[.2f]{test.roc.ci[3]}]")

legend("bottomright", 
       legend = c(roc.test.legend.text, roc.train.legend.text), 
       col = c(salmon, lightblue),
       lwd = 2)
dev.off()
```


# Precision-recall curves
```{r}
# build precision-recall curves

pr.test <- pr.curve(scores.class0=test$proba, weights.class0=test$target, curve=T,
                    rand.compute = TRUE)
pr.train <- pr.curve(scores.class0=train$proba, weights.class0=train$target, curve=T,
                     rand.compute = TRUE)

test.curve.points <- pr.test$curve
train.curve.points <- pr.train$curve

#plot(pr.test, auc.main=FALSE, color='salmon', main=NULL, sub=NULL)
#plot(pr.train, auc.main=FALSE, add=TRUE, color='lightsteelblue', main=NULL, sub=NULL)
```


```{r}
# bootstrap AUCs for 95% CI
set.seed(1303)

fci<-function(data,indices,x,y){
   d<-as.data.frame(data[indices,])
   r<-MLmetrics::PRAUC(d[,2], d[,1])
   r
}

bootout.test <- boot(data=test,
              x=test$target,
              y=test$proba,
              R=2000,
              statistic=fci
              )

bootout.train <- boot(data=train,
              x=train$target,
              y=train$proba,
              R=2000,
              statistic=fci
              )

prauc.ci.test <- boot.ci(bootout.test, type="perc")
prauc.ci.train <- boot.ci(bootout.train, type="perc")
```


```{r}
# compute proprtion of positives in train/test samples for PR curve baseline
npos.test = length(which(test$target == 1))
nneg.test = length(which(test$target == 0))

npos.train = length(which(train$target == 1))
nneg.train = length(which(train$target == 0))

prop.pos.test = npos.test / (npos.test + nneg.test)
prop.pos.train = npos.train / (npos.train + nneg.train)

prop.pos.test
prop.pos.train
```

```{r}
# plot PR curve alone

pdf("../output/plots/pr.pdf", width = 6, height=6)
# precision-recall curves
par(mar=c(6.1, 4.1, 4.1, 2.1), pty="s")
plot(test.curve.points[,1], test.curve.points[,2], xlab="Recall", ylab="Precision", t="l",
     ylim=c(0.0,1.0),
     lwd=3, col=salmon, asp=1, 
     ann=FALSE)
lines(train.curve.points[,1], train.curve.points[,2], xlab="Recall", ylab="Precision", t="l",
      ylim=c(0.0,1.0),
     lwd=3, col=lightblue)
abline(h=prop.pos.train, col="gray", lty=1)
title("Precision-Recall curves", line=1.8)
mtext('Recall', side=1, line=2.5, adj=0.5) # x axis
mtext('Precision', side=2, line=2.5, adj=0.5) # y axis

pr.train.legend.text <- stringr::str_interp("Train AUC $[.2f]{prauc.ci.train$t0} [$[.2f]{prauc.ci.train$percent[4]}-$[.2f]{prauc.ci.train$percent[5]}]")
pr.test.legend.text <- stringr::str_interp("Test  AUC $[.2f]{prauc.ci.test$t0} [$[.2f]{prauc.ci.test$percent[4]}-$[.2f]{prauc.ci.test$percent[5]}]")

legend("bottomright", 
       legend = c(pr.test.legend.text, pr.train.legend.text), 
       col = c(salmon, lightblue),
       lwd = 2)
dev.off()
```


# Plot ROC and PR curves, side by side
```{r}
pdf("../output/plots/roc_pr.pdf", width = 12)

par(mfcol=c(1,2), pty="s")

# ROC curves
test.roc <- pROC::roc(test$target, test$proba, ci=TRUE, plot=TRUE,
    legacy.axes=TRUE, lwd=3, col=salmon)
title("ROC curves", line=2.3)

test.roc.auc <- round(pROC::auc(test.roc), 2)
test.roc.ci <- pROC::ci.auc(test.roc, method="bootstrap")

train.roc <- pROC::roc(train$target, train$proba, ci=TRUE, add=TRUE, plot=TRUE,
    legacy.axes=TRUE, lwd=3, col=lightblue)

train.roc.auc <- round(pROC::auc(train.roc), 2)
train.roc.ci <- pROC::ci.auc(train.roc, method="bootstrap")

roc.train.legend.text <- stringr::str_interp("Train AUC $[.2f]{train.roc.auc} [$[.2f]{train.roc.ci[1]}-$[.2f]{train.roc.ci[3]}]")
roc.test.legend.text <- stringr::str_interp("Test  AUC $[.2f]{test.roc.auc} [$[.2f]{test.roc.ci[1]}-$[.2f]{test.roc.ci[3]}]")

legend("bottomright", 
       legend = c(roc.test.legend.text, roc.train.legend.text), 
       col = c(salmon, lightblue),
       lwd = 2)

# precision-recall curves
par(mar=c(6.1, 4.1, 4.1, 2.1))
plot(test.curve.points[,1], test.curve.points[,2], xlab="Recall", ylab="Precision", t="l",
     ylim=c(0.0,1.0),
     lwd=3, col=salmon, asp=1, 
     ann=FALSE)
lines(train.curve.points[,1], train.curve.points[,2], xlab="Recall", ylab="Precision", t="l",
      ylim=c(0.0,1.0),
     lwd=3, col=lightblue)
abline(h=prop.pos.train, col="gray", lty=1)
title("Precision-Recall curves", line=1.8)
mtext('Recall', side=1, line=2.5, adj=0.5) # x axis
mtext('Precision', side=2, line=2.5, adj=0.5) # y axis

pr.train.legend.text <- stringr::str_interp("Train AUC $[.2f]{prauc.ci.train$t0} [$[.2f]{prauc.ci.train$percent[4]}-$[.2f]{prauc.ci.train$percent[5]}]")
pr.test.legend.text <- stringr::str_interp("Test  AUC $[.2f]{prauc.ci.test$t0} [$[.2f]{prauc.ci.test$percent[4]}-$[.2f]{prauc.ci.test$percent[5]}]")

legend("bottomright", 
       legend = c(pr.test.legend.text, pr.train.legend.text), 
       col = c(salmon, lightblue),
       lwd = 2)

dev.off()
```

# Confusion Matrix
```{r}
# threshold of 0.5 allows to have a sensitivity of 90% and a specificity of 42%
test.coords <- pROC::coords(roc=test.roc, x = "all", transpose = FALSE)
test.coords[test.coords$sensitivity >= .75, ]
```

```{r}
# Build confusion matrix using the cvms package
test.copy <- data.frame(test)

# convert predicted probabilities to labels using the chosen threshold
threshold <- 0.5008254
test.copy$labels <- ifelse(test.copy$proba > threshold, 1, 0)

test.copy$target <- ifelse(test.copy$target == "1", "Symptomatic", "Asymptomatic")
test.copy$labels <- ifelse(test.copy$labels == "1", "Symptomatic", "Asymptomatic")

eval <- cvms::evaluate(test.copy, target_col = "target", prediction_cols = "labels",
                       type="binomial")
```


```{r}
# Plot confusion matrix alone

pdf("../output/plots/conf_matrix.pdf", width=6, height=6)
cm.plot <- cvms::plot_confusion_matrix(eval, 
                                       target_col = "Actual",
                                       prediction_col = "Predicted",
                                       add_row_percentages = FALSE,
                                       add_col_percentages = FALSE,
                                       palette="Blues",
                                       font_counts = cvms::font(size=6, 
                                                                color="#342d2d", 
                                                                vjust=0),
                                       font_normalized = cvms::font(size=7, 
                                                                    color="#342d2d", 
                                                                    vjust=-0.5)) +
  ggplot2::ggtitle("Predicted and actual events") +
  ggplot2::theme(axis.title = ggplot2::element_text(size=16),
                 axis.title.y = ggplot2::element_text(margin = ggplot2::margin(r=25)),
                 axis.text.x = ggplot2::element_text(size=13),
                 axis.text.y = ggplot2::element_text(size=13),
                 axis.ticks.length = ggplot2::unit(-1, "cm"),
                 plot.title = ggplot2::element_text(size=17, 
                                                    hjust=0.5, 
                                                    vjust=4.0,
                                                    family = "sans",
                                                    face="bold")
                 ) +
  ggplot2::labs(x = "Actual", y = "Predicted")

cm.plot

dev.off()
```



# Final plot: ROC curves, PR curves and confusion matrix in one row

```{r}

pdf("../output/plots/roc_pr_cm.pdf", width = 18, height=7.7)

par(mfrow=c(1,3), pty="s")

# Plot ROC curves
test.roc <- pROC::roc(test$target, test$proba, ci=TRUE, plot=TRUE,
    legacy.axes=TRUE, lwd=3, col=salmon,
    cex.lab=2.0,
    cex.axis=1.5,
    ann=FALSE)
title(main="ROC curves", line=2.3, cex.main=2.0) 

test.roc.auc <- round(pROC::auc(test.roc), 2)
test.roc.ci <- pROC::ci.auc(test.roc, method="bootstrap")

train.roc <- pROC::roc(train$target, train$proba, ci=TRUE, add=TRUE, plot=TRUE,
    legacy.axes=TRUE, lwd=3, col=lightblue)

train.roc.auc <- round(pROC::auc(train.roc), 2)
train.roc.ci <- pROC::ci.auc(train.roc, method="bootstrap")

mtext('1 - Specificity', side=1, line=3.0, adj=0.5, cex=1.5) # x axis
mtext('Sensitivity', side=2, line=2.5, adj=0.5, cex=1.5) # y axis

roc.train.legend.text <- stringr::str_interp("Train AUC $[.2f]{train.roc.auc} [$[.2f]{train.roc.ci[1]}-$[.2f]{train.roc.ci[3]}]")
roc.test.legend.text <- stringr::str_interp("Test  AUC $[.2f]{test.roc.auc} [$[.2f]{test.roc.ci[1]}-$[.2f]{test.roc.ci[3]}]")

legend("bottomright", 
       legend = c(roc.test.legend.text, roc.train.legend.text), 
       col = c(salmon, lightblue),
       lwd = 2,
       cex=2)

mtext("A", adj=0, line=1.5,  cex=1.5, font=2)

# Plot Precision-Recall curves
par(mar=c(6.1, 4.1, 4.1, 2.1))

plot(test.curve.points[,1], test.curve.points[,2], 
     xlab="Recall", ylab="Precision", 
     t="l",
     ylim=c(0.0,1.0),
     lwd=3, col=salmon, asp=1,
     cex.lab=2.0,
     cex.axis=1.5,
     mgp=c(2.2,0.5,0),
     ann=FALSE) #mgp allows moving axis labels further/closer to the xticks (first arg), 2nd arg deals with xticks labels
lines(train.curve.points[,1], train.curve.points[,2], 
      xlab="Recall", 
      ylab="Precision", 
      t="l",
      ylim=c(0.0,1.0),
      lwd=3, col=lightblue)

abline(h=prop.pos.train, col="gray", lty=1)
title(main="Precision-Recall curves", line=1.8, cex.main=2.0)
mtext('Recall', side=1, line=3.2, adj=0.5, cex=1.5) # x axis
mtext('Precision', side=2, line=3.0, adj=0.5, cex=1.5) # y axis

pr.train.legend.text <- stringr::str_interp("Train AUC $[.2f]{prauc.ci.train$t0} [$[.2f]{prauc.ci.train$percent[4]}-$[.2f]{prauc.ci.train$percent[5]}]")
pr.test.legend.text <- stringr::str_interp("Test  AUC $[.2f]{prauc.ci.test$t0} [$[.2f]{prauc.ci.test$percent[4]}-$[.2f]{prauc.ci.test$percent[5]}]")

legend("bottomright", 
       legend = c(pr.test.legend.text, pr.train.legend.text), 
       col = c(salmon, lightblue),
       lwd = 2,
       cex=2)

mtext("B", adj=0, line=1.5,  cex=1.5, font=2)


mtext(text="Assessment of model performance", side = 1, line = -46, cex=2.0)


# confusion matrix
plot.new()              ## suggested by @Josh
vps <- baseViewports()
pushViewport(vps$figure) ##   I am in the space of the autocorrelation plot
vp1 <-plotViewport(c(1.8,1,0,1))

print(cm.plot, vp=vp1)
mtext("C", adj=0, line=1.5,  cex=1.5, font=2)

dev.off()
```

